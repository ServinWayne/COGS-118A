{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from algorithms.sort import quick_sort\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "from scipy import stats\n",
    "import seaborn as sns\n",
    "from sklearn import svm \n",
    "from sklearn import tree\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import GridSearchCV \n",
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn.preprocessing import StandardScaler \n",
    "from sklearn.svm import SVC \n",
    "from sklearn.utils import shuffle\n",
    "from random import randint\n",
    "import statistics\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ***NOTE***: \n",
    "I have suppressed/removed my code outputs for this notebook per algo/dataset. This is to shorten the output of my notebook from what would be 548 pages to somewhere under 40.  \n",
    "\n",
    "\n",
    "## Importing The Datasets:\n",
    "\n",
    "Before we go any further, lets print the heads of our datasets so we can see what we are working with!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing Dry Bean DataSet.\n",
    "beanData = pd.read_excel('Dry_Bean_Dataset.xlsx')\n",
    "\n",
    "# Importing the Skin_NonSkin dataset.\n",
    "skinData = pd.read_excel('skin_NonSkin1.xlsx')\n",
    "\n",
    "#Importing the Adult dataset.\n",
    "adultData = pd.read_csv(\"adult.data\")\n",
    "\n",
    "# Importing the Letter dataset.\n",
    "letterData = pd.read_csv(\"letter-recognition.data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Area</th>\n",
       "      <th>Perimeter</th>\n",
       "      <th>MajorAxisLength</th>\n",
       "      <th>MinorAxisLength</th>\n",
       "      <th>AspectRation</th>\n",
       "      <th>Eccentricity</th>\n",
       "      <th>ConvexArea</th>\n",
       "      <th>EquivDiameter</th>\n",
       "      <th>Extent</th>\n",
       "      <th>Solidity</th>\n",
       "      <th>roundness</th>\n",
       "      <th>Compactness</th>\n",
       "      <th>ShapeFactor1</th>\n",
       "      <th>ShapeFactor2</th>\n",
       "      <th>ShapeFactor3</th>\n",
       "      <th>ShapeFactor4</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>28395</td>\n",
       "      <td>610.291</td>\n",
       "      <td>208.178117</td>\n",
       "      <td>173.888747</td>\n",
       "      <td>1.197191</td>\n",
       "      <td>0.549812</td>\n",
       "      <td>28715</td>\n",
       "      <td>190.141097</td>\n",
       "      <td>0.763923</td>\n",
       "      <td>0.988856</td>\n",
       "      <td>0.958027</td>\n",
       "      <td>0.913358</td>\n",
       "      <td>0.007332</td>\n",
       "      <td>0.003147</td>\n",
       "      <td>0.834222</td>\n",
       "      <td>0.998724</td>\n",
       "      <td>SEKER</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>28734</td>\n",
       "      <td>638.018</td>\n",
       "      <td>200.524796</td>\n",
       "      <td>182.734419</td>\n",
       "      <td>1.097356</td>\n",
       "      <td>0.411785</td>\n",
       "      <td>29172</td>\n",
       "      <td>191.272750</td>\n",
       "      <td>0.783968</td>\n",
       "      <td>0.984986</td>\n",
       "      <td>0.887034</td>\n",
       "      <td>0.953861</td>\n",
       "      <td>0.006979</td>\n",
       "      <td>0.003564</td>\n",
       "      <td>0.909851</td>\n",
       "      <td>0.998430</td>\n",
       "      <td>SEKER</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>29380</td>\n",
       "      <td>624.110</td>\n",
       "      <td>212.826130</td>\n",
       "      <td>175.931143</td>\n",
       "      <td>1.209713</td>\n",
       "      <td>0.562727</td>\n",
       "      <td>29690</td>\n",
       "      <td>193.410904</td>\n",
       "      <td>0.778113</td>\n",
       "      <td>0.989559</td>\n",
       "      <td>0.947849</td>\n",
       "      <td>0.908774</td>\n",
       "      <td>0.007244</td>\n",
       "      <td>0.003048</td>\n",
       "      <td>0.825871</td>\n",
       "      <td>0.999066</td>\n",
       "      <td>SEKER</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>30008</td>\n",
       "      <td>645.884</td>\n",
       "      <td>210.557999</td>\n",
       "      <td>182.516516</td>\n",
       "      <td>1.153638</td>\n",
       "      <td>0.498616</td>\n",
       "      <td>30724</td>\n",
       "      <td>195.467062</td>\n",
       "      <td>0.782681</td>\n",
       "      <td>0.976696</td>\n",
       "      <td>0.903936</td>\n",
       "      <td>0.928329</td>\n",
       "      <td>0.007017</td>\n",
       "      <td>0.003215</td>\n",
       "      <td>0.861794</td>\n",
       "      <td>0.994199</td>\n",
       "      <td>SEKER</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>30140</td>\n",
       "      <td>620.134</td>\n",
       "      <td>201.847882</td>\n",
       "      <td>190.279279</td>\n",
       "      <td>1.060798</td>\n",
       "      <td>0.333680</td>\n",
       "      <td>30417</td>\n",
       "      <td>195.896503</td>\n",
       "      <td>0.773098</td>\n",
       "      <td>0.990893</td>\n",
       "      <td>0.984877</td>\n",
       "      <td>0.970516</td>\n",
       "      <td>0.006697</td>\n",
       "      <td>0.003665</td>\n",
       "      <td>0.941900</td>\n",
       "      <td>0.999166</td>\n",
       "      <td>SEKER</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Area  Perimeter  MajorAxisLength  MinorAxisLength  AspectRation  \\\n",
       "0  28395    610.291       208.178117       173.888747      1.197191   \n",
       "1  28734    638.018       200.524796       182.734419      1.097356   \n",
       "2  29380    624.110       212.826130       175.931143      1.209713   \n",
       "3  30008    645.884       210.557999       182.516516      1.153638   \n",
       "4  30140    620.134       201.847882       190.279279      1.060798   \n",
       "\n",
       "   Eccentricity  ConvexArea  EquivDiameter    Extent  Solidity  roundness  \\\n",
       "0      0.549812       28715     190.141097  0.763923  0.988856   0.958027   \n",
       "1      0.411785       29172     191.272750  0.783968  0.984986   0.887034   \n",
       "2      0.562727       29690     193.410904  0.778113  0.989559   0.947849   \n",
       "3      0.498616       30724     195.467062  0.782681  0.976696   0.903936   \n",
       "4      0.333680       30417     195.896503  0.773098  0.990893   0.984877   \n",
       "\n",
       "   Compactness  ShapeFactor1  ShapeFactor2  ShapeFactor3  ShapeFactor4  Class  \n",
       "0     0.913358      0.007332      0.003147      0.834222      0.998724  SEKER  \n",
       "1     0.953861      0.006979      0.003564      0.909851      0.998430  SEKER  \n",
       "2     0.908774      0.007244      0.003048      0.825871      0.999066  SEKER  \n",
       "3     0.928329      0.007017      0.003215      0.861794      0.994199  SEKER  \n",
       "4     0.970516      0.006697      0.003665      0.941900      0.999166  SEKER  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Showing first five rows of the beanData dataset.\n",
    "beanData.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>74</th>\n",
       "      <th>85</th>\n",
       "      <th>123</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>73</td>\n",
       "      <td>84</td>\n",
       "      <td>122</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>72</td>\n",
       "      <td>83</td>\n",
       "      <td>121</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>70</td>\n",
       "      <td>81</td>\n",
       "      <td>119</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>70</td>\n",
       "      <td>81</td>\n",
       "      <td>119</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>69</td>\n",
       "      <td>80</td>\n",
       "      <td>118</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   74   85   123  1  \n",
       "0   73   84  122    1\n",
       "1   72   83  121    1\n",
       "2   70   81  119    1\n",
       "3   70   81  119    1\n",
       "4   69   80  118    1"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Showing first five rows of the skinData dataset.\n",
    "skinData.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>39</th>\n",
       "      <th>State-gov</th>\n",
       "      <th>77516</th>\n",
       "      <th>Bachelors</th>\n",
       "      <th>13</th>\n",
       "      <th>Never-married</th>\n",
       "      <th>Adm-clerical</th>\n",
       "      <th>Not-in-family</th>\n",
       "      <th>White</th>\n",
       "      <th>Male</th>\n",
       "      <th>2174</th>\n",
       "      <th>0</th>\n",
       "      <th>40</th>\n",
       "      <th>United-States</th>\n",
       "      <th>&lt;=50K</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>50</td>\n",
       "      <td>Self-emp-not-inc</td>\n",
       "      <td>83311</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>13</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Exec-managerial</td>\n",
       "      <td>Husband</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>38</td>\n",
       "      <td>Private</td>\n",
       "      <td>215646</td>\n",
       "      <td>HS-grad</td>\n",
       "      <td>9</td>\n",
       "      <td>Divorced</td>\n",
       "      <td>Handlers-cleaners</td>\n",
       "      <td>Not-in-family</td>\n",
       "      <td>White</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>53</td>\n",
       "      <td>Private</td>\n",
       "      <td>234721</td>\n",
       "      <td>11th</td>\n",
       "      <td>7</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Handlers-cleaners</td>\n",
       "      <td>Husband</td>\n",
       "      <td>Black</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>28</td>\n",
       "      <td>Private</td>\n",
       "      <td>338409</td>\n",
       "      <td>Bachelors</td>\n",
       "      <td>13</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Prof-specialty</td>\n",
       "      <td>Wife</td>\n",
       "      <td>Black</td>\n",
       "      <td>Female</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>Cuba</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>37</td>\n",
       "      <td>Private</td>\n",
       "      <td>284582</td>\n",
       "      <td>Masters</td>\n",
       "      <td>14</td>\n",
       "      <td>Married-civ-spouse</td>\n",
       "      <td>Exec-managerial</td>\n",
       "      <td>Wife</td>\n",
       "      <td>White</td>\n",
       "      <td>Female</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>40</td>\n",
       "      <td>United-States</td>\n",
       "      <td>&lt;=50K</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   39          State-gov   77516   Bachelors   13        Never-married  \\\n",
       "0  50   Self-emp-not-inc   83311   Bachelors   13   Married-civ-spouse   \n",
       "1  38            Private  215646     HS-grad    9             Divorced   \n",
       "2  53            Private  234721        11th    7   Married-civ-spouse   \n",
       "3  28            Private  338409   Bachelors   13   Married-civ-spouse   \n",
       "4  37            Private  284582     Masters   14   Married-civ-spouse   \n",
       "\n",
       "         Adm-clerical   Not-in-family   White     Male   2174   0   40  \\\n",
       "0     Exec-managerial         Husband   White     Male      0   0   13   \n",
       "1   Handlers-cleaners   Not-in-family   White     Male      0   0   40   \n",
       "2   Handlers-cleaners         Husband   Black     Male      0   0   40   \n",
       "3      Prof-specialty            Wife   Black   Female      0   0   40   \n",
       "4     Exec-managerial            Wife   White   Female      0   0   40   \n",
       "\n",
       "    United-States   <=50K  \n",
       "0   United-States   <=50K  \n",
       "1   United-States   <=50K  \n",
       "2   United-States   <=50K  \n",
       "3            Cuba   <=50K  \n",
       "4   United-States   <=50K  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Showing first fiver rows of the adultData dataset.\n",
    "adultData.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>T</th>\n",
       "      <th>2</th>\n",
       "      <th>8</th>\n",
       "      <th>3</th>\n",
       "      <th>5</th>\n",
       "      <th>1</th>\n",
       "      <th>8.1</th>\n",
       "      <th>13</th>\n",
       "      <th>0</th>\n",
       "      <th>6</th>\n",
       "      <th>6.1</th>\n",
       "      <th>10</th>\n",
       "      <th>8.2</th>\n",
       "      <th>0.1</th>\n",
       "      <th>8.3</th>\n",
       "      <th>0.2</th>\n",
       "      <th>8.4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I</td>\n",
       "      <td>5</td>\n",
       "      <td>12</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>13</td>\n",
       "      <td>3</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>D</td>\n",
       "      <td>4</td>\n",
       "      <td>11</td>\n",
       "      <td>6</td>\n",
       "      <td>8</td>\n",
       "      <td>6</td>\n",
       "      <td>10</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>N</td>\n",
       "      <td>7</td>\n",
       "      <td>11</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>10</td>\n",
       "      <td>6</td>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>G</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>S</td>\n",
       "      <td>4</td>\n",
       "      <td>11</td>\n",
       "      <td>5</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>6</td>\n",
       "      <td>9</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>9</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   T  2   8  3  5  1  8.1  13  0  6  6.1  10  8.2  0.1  8.3  0.2  8.4\n",
       "0  I  5  12  3  7  2   10   5  5  4   13   3    9    2    8    4   10\n",
       "1  D  4  11  6  8  6   10   6  2  6   10   3    7    3    7    3    9\n",
       "2  N  7  11  6  6  3    5   9  4  6    4   4   10    6   10    2    8\n",
       "3  G  2   1  3  1  1    8   6  6  6    6   5    9    1    7    5   10\n",
       "4  S  4  11  5  8  3    8   8  6  9    5   6    6    0    8    9    7"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Showing first fiver rows of the letter dataset.\n",
    "letterData.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning:\n",
    "\n",
    "As you can see, most of these datasets are in no condition to just start running classification algorithms on. Some have no binary classification column, others have a wide mix of strings and numerical values, and some don't even have their correct column names. I will have to clean them up and give each of them at least one binary column to perform my calculations on. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Cleaning beanData:\n",
    "# First, I will drop the \"class\" column from the Dry Bean Dataset. It serves \n",
    "# no purpose for our calculations as I will be using Area column for my \n",
    "# predictions.\n",
    "beanData = beanData.drop('Class', axis = 1)\n",
    "# Next, I find the median of the Area column.\n",
    "med = statistics.median(beanData['Area'])\n",
    "# Using this median, I set any value above it to 1, and any value \n",
    "# below it to 0.\n",
    "beanData['Area'] = beanData['Area'].mask(beanData['Area'] > med, 1)\n",
    "beanData['Area'] = beanData['Area'].mask(beanData['Area'] > 1, 0)\n",
    "\n",
    "\n",
    "## Cleaning skinData:\n",
    "# shuffling skin data so class column contain both skin and non skin data.\n",
    "skinData = shuffle(skinData)\n",
    "# Selecting only the first 50,859 attributes of skinData to classify. \n",
    "skinData = skinData[1:50860] \n",
    "# Giving skinData the correct column names it's supposed to have \n",
    "# (by default it has none).\n",
    "skinData.columns = ['R', 'G', 'B', 'Y']\n",
    "\n",
    "\n",
    "## Cleaning adultData:\n",
    "# Giving adultData the correct column names it's supposed to have \n",
    "# (when i inported it, it had none).\n",
    "adultData.columns = ['age', 'workclass', 'fnlwgt', 'education', \n",
    "                     'education-num', 'marital-status', 'occupation', \n",
    "                     'relationship', 'race', 'sex', 'capital-gain', \n",
    "                     'capital-loss', 'hours-per-week', 'country', \n",
    "                     'salary']\n",
    "# Defining adultData as a list which I will use later on in cleaning.\n",
    "df_list = [adultData]\n",
    "\n",
    "# Encoding >50K string as a 0 and <=50K string as a 1 in the salary \n",
    "# column using the map function.\n",
    "salaryDict={' <=50K': 1,' >50K': 0}\n",
    "adultData['salary'] = adultData['salary'].map(salaryDict).astype(int)\n",
    "\n",
    "# Encoding Male as 1 and Female as 0 in the sex column using the map function.\n",
    "adultData['sex'] = adultData['sex'].map({' Male':1,' Female':0}).astype(int)\n",
    "\n",
    "# Replacing any missing data from the country column as a numpy NaN.\n",
    "adultData['country'] = adultData['country'].replace(' ?', np.nan)\n",
    "# Replacing any missing data from the workclass column as a numpy NaN.\n",
    "adultData['workclass'] = adultData['workclass'].replace(' ?', np.nan)\n",
    "# Replacing any missing data from the occupation column as a numpy NaN.\n",
    "adultData['occupation'] = adultData['occupation'].replace(' ?', np.nan)\n",
    "adultData.dropna(how = 'any', inplace = True)\n",
    "\n",
    "# Turning adultData into a list to assign universal names to the different \n",
    "# strings in that column using loc. \n",
    "for countries in df_list:\n",
    "    countries.loc[countries['country'] != ' United-States', 'country'] \n",
    "    = 'Other_Countries'\n",
    "    countries.loc[countries['country'] == ' United-States', 'country'] \n",
    "    = 'United_States'\n",
    "    \n",
    "# Assigning 1 to the universalized UnitedStates string and 0 to the others.\n",
    "adultData['country'] = adultData['country'].map({'United_States': 1,\n",
    "                                                 'Other_Countries':0}).astype(int)\n",
    "\n",
    "# Turning different strings in the marital-status column to two universal \n",
    "# strings: Single and Dating. \n",
    "adultData['marital-status'] = adultData['marital-status'].replace(\n",
    "    [' Divorced',' Married-spouse-absent',' Never-married',' Separated', \n",
    "     ' Widowed'], 'Single')\n",
    "adultData['marital-status'] = adultData['marital-status'].replace(\n",
    "    [' Married-AF-spouse',' Married-civ-spouse'], 'Dating')\n",
    "# Assigning the two universalized variables equal to 0 if they're dating \n",
    "# and 1 if they're single.\n",
    "adultData['marital-status'] = adultData['marital-status'].map({'Dating': 0,\n",
    "                                                               'Single': 1})\n",
    "\n",
    "# Creating a dictionary where strings are encoded as numerical values, \n",
    "# and use it in a map function for relationship.\n",
    "relationshipDict = {' Unmarried': 0,' Wife': 1,' Husband': 2,\n",
    "                    ' Not-in-family': 3,' Own-child': 4,\n",
    "                    ' Other-relative': 5}\n",
    "adultData['relationship'] = adultData['relationship'].map(relationshipDict)\n",
    "\n",
    "# Creating a dictionary where strings are encoded as numerical values, \n",
    "# and use it in a map function for race.\n",
    "raceDict = {' White':0,' Amer-Indian-Eskimo':1,\n",
    "            ' Asian-Pac-Islander':2,' Black':3,' Other':4}\n",
    "adultData['race']= adultData['race'].map(raceDict)\n",
    "\n",
    "# Creating a function which will go through inputs of a column, \n",
    "# and assign every string to a universalizing term. \n",
    "def categorize(i):\n",
    "    if i['workclass'] == ' Federal-gov' \n",
    "    or i['workclass'] == ' Local-gov' \n",
    "    or i['workclass'] ==' State-gov': \n",
    "        return 'government'\n",
    "    elif i['workclass'] == ' Private': \n",
    "        return 'private'\n",
    "    elif i['workclass'] == ' Self-emp-inc' \n",
    "    or i['workclass'] == ' Self-emp-not-inc': \n",
    "        return 'self_employed'\n",
    "    else: \n",
    "        return 'unemployed'\n",
    "\n",
    "# For some reason the workclass column was giving me trouble when I tried to \n",
    "# override it with the new strings. To save precious time and go around hours \n",
    "# of debugging, I'll just run the function on a new column job_type, and \n",
    "# remove workclass column later. This will transfer all of the new data to \n",
    "# a new column with a new name. \n",
    "adultData['job_type'] = adultData.apply(categorize, axis = 1)\n",
    "employmentDict = {'government': 0, 'private': 1, 'self_employed': 2, \n",
    "                  'unemployed': 3}\n",
    "adultData['job_type'] = adultData['job_type'].map(employmentDict)\n",
    "\n",
    "# Dropping workclass, education, and occupation columns as they don't serve \n",
    "# a purpose to my analysis. \n",
    "adultData.drop(labels=['workclass','education','occupation'], \n",
    "               axis = 1, inplace = True)\n",
    "\n",
    "# Finally, using loc on capital-gain column to assign 1 to values higher than \n",
    "# 0, and leaving 0 as 0.\n",
    "adultData.loc[(adultData['capital-gain'] > 0),'capital-gain'] = 1\n",
    "adultData.loc[(adultData['capital-gain'] == 0 ,'capital-gain')]= 0\n",
    "\n",
    "\n",
    "## Cleaning out letterData.\n",
    "# Assigning the correct column names to the dataset \n",
    "# (when I imported it, none came with it.)\n",
    "letterData.columns = ['letter', 'x-box', 'y-box', 'width', 'high', 'onpix', \n",
    "                      'x-bar', 'y-bar', 'x2bar', 'y2bar', 'xybar', 'x2ybr', \n",
    "                      'xy2b', 'x-ege', 'xegvy', 'y-ege', 'yegvx']\n",
    "\n",
    "letterData['letter'] = letterData['letter'].replace(['A','B','C','D','E','F',\n",
    "                                                     'G','H','I','J','K','L',\n",
    "                                                     'M'], 'FirstHalf')\n",
    "letterData['letter'] = letterData['letter'].replace(['N','O','P','Q','R','S',\n",
    "                                                     'T','U','V','W','X','Y',\n",
    "                                                     'Z'], 'SecondHalf')\n",
    "\n",
    "# Assigning the two universalized variables equal to 0 if they're dating and \n",
    "# 1 if they're single.\n",
    "letterData['letter'] = letterData['letter'].map({'FirstHalf': 0,\n",
    "                                                 'SecondHalf': 1})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Analysis:\n",
    "\n",
    "For this section, what I did was create a massive for loop which ran through 10 iterations (Trials). What I did next is slightly different than the CNM06 paper. Essentially, instead of randomly choosing 5000 datapoints per trial to split for training/testing, what I did was for each one of those iterations, I had my code split my data into 4 different brackets of training and testing ( 80/20, 70/30, 60/40, and 50/50). This gave me much larger data samples to work with and increased my statistical power by physically having more data to work with, thus giving me more accurate results. To eliminated the worry of selecting 5000 data points, I only got datasets that were at the minimum, 10,000 instances. Thus, when I split my data into 80/20, 70/30, 60/40, and 50/50, I would have more than 5000 data points for those iterrations per trial. Thus, since all 4 of my splits were over 5,000 points, I can say I not only met the project requirements but also did more training!\n",
    "\n",
    "Afterwards, for each split my data has, my loop would take that Xtrain and Ytrain and run my four algorithms on them. As for my algorithms, I chose to do three of them: Decision Trees, SVMs, and Random Forests. I used sklearn for each algorithm and used their code to let pyton choose the best parameters for each iteration! I will repeat this loop 4 times for the 4 different datasets, and record their average scores across accuracy, fi-score, mean, precision, and more. \n",
    "\n",
    "The majority of the code I will be using will come from the homework assignments on said algorithms, as well as code/content we discussed in lectures/ discussions. The other place I refered to for learning how to do some of these algorithms and packages is the scikit-learn website, which I will link in my references. \n",
    "\n",
    "By the end of all this, my algorithm would have run 10 iteration x 4 different data splits x 3 algorithms x 4 datasets which equals a total of 480 trials! (Compared to the 80 trials needed for the project). \n",
    "\n",
    "### Dry Bean Dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# The following code in this block is for my Dry Bean Dataset:\n",
    "\n",
    "# We begin by making a massive for-loop to go through 10 trials.\n",
    "for iterations in range(10):\n",
    "    \n",
    "    # Shuffling the dataset.\n",
    "    beanData = shuffle(beanData) \n",
    "    \n",
    "    # Setting up X values to be all variables except the area, and Y values to \n",
    "    # be the area column.\n",
    "    X = beanData.drop('Area', axis = 1)  \n",
    "    Y = beanData['Area'] \n",
    "    \n",
    "    # Printing the number of each iteration as we go up.\n",
    "    print(\"Trial Number:\", iterations + 1)  \n",
    "\n",
    "    \n",
    "    # Setting up a for loop within my iterations loop, which will split my \n",
    "    # data 4 times, in different ways.\n",
    "    for split in range(4):\n",
    "        if split == 0:\n",
    "            # For the first split, I will do a 80(training)/20(testing) \n",
    "            # split, with a Random State of 50.\n",
    "            X_train, X_test, Y_train, Y_test = \n",
    "            train_test_split(X, Y, test_size = 0.2, random_state = 50)\n",
    "            \n",
    "            # Standardizing features by removing the mean and scaling to a \n",
    "            # unit variance.\n",
    "            stsc = StandardScaler() \n",
    "            \n",
    "            # Transforming X train and X test based on the \n",
    "            # standardization above.\n",
    "            X_train = stsc.fit_transform(X_train)\n",
    "            X_test = stsc.fit_transform(X_test)\n",
    "            print(\"Data Split 80/20:\")\n",
    "        \n",
    "        if split == 1:\n",
    "            # For the second split, I will do a 70(training)/30(testing)\n",
    "            # split, with a Random State of 50.\n",
    "            X_train, X_test, Y_train, Y_test = \n",
    "            train_test_split(X, Y, test_size = 0.3, random_state = 50)\n",
    "            \n",
    "            # Standardizing features by removing the mean and scaling to a \n",
    "            # unit variance.\n",
    "            stsc = StandardScaler() \n",
    "            \n",
    "            # Transforming X train and X test based on the \n",
    "            # standardization above.\n",
    "            X_train = stsc.fit_transform(X_train) \n",
    "            X_test = stsc.fit_transform(X_test)\n",
    "            print(\"Data Split 70/30:\")\n",
    "\n",
    "        if split == 2:\n",
    "            # For the third split, I will do a 60(training)/40(testing) \n",
    "            # split, with a Random State of 50.\n",
    "            X_train, X_test, Y_train, Y_test = \n",
    "            train_test_split(X, Y, test_size = 0.4, random_state = 50)\n",
    "            \n",
    "            # Standardizing features by removing the mean and scaling to a \n",
    "            # unit variance.\n",
    "            stsc = StandardScaler() \n",
    "            \n",
    "            # Transforming X train and X test based on the \n",
    "            # standardization above.\n",
    "            X_train = stsc.fit_transform(X_train) \n",
    "            X_test = stsc.fit_transform(X_test)\n",
    "            print(\"Data Split 60/40:\")\n",
    "            \n",
    "        if split == 3:\n",
    "            # For the fourth split, I will do a 50(training)/50(testing) \n",
    "            # split, with a Random State of 50.\n",
    "            X_train, X_test, Y_train, Y_test = \n",
    "            train_test_split(X, Y, test_size = 0.5, random_state = 50)\n",
    "            \n",
    "            # Standardizing features by removing the mean and scaling to a \n",
    "            # unit variance.\n",
    "            stsc = StandardScaler()\n",
    "        \n",
    "            # Transforming X train and X test based on the \n",
    "            # standardization above.\n",
    "            X_train = stsc.fit_transform(X_train) \n",
    "            X_test = stsc.fit_transform(X_test) \n",
    "            print(\"Data Split 50/50:\")\n",
    "\n",
    "            \n",
    "        # Setting up a for-loop for my 3 algorithms. \n",
    "        for alg in range(3):\n",
    "            ## The first group of algorithms will be Decision Trees .\n",
    "            if alg == 0:\n",
    "                # The first will be a Decision Tree with entropy.\n",
    "                # Setting a list for depth length and creating a Classifier \n",
    "                # with entropy.\n",
    "                Depth_list = [1, 4, 8, 16] \n",
    "                classifierENT = tree.DecisionTreeClassifier(criterion = \n",
    "                                                            'entropy') \n",
    "                \n",
    "                # Here, I will select hyperparameters via a systematic \n",
    "                # gridsearch, with a cross validation score of 10 and \n",
    "                # scoring method set to accuracy:\n",
    "                GridSearchENT = GridSearchCV(classifierENT, \n",
    "                                             {'max_depth':Depth_list},\n",
    "                                             scoring = 'accuracy', \n",
    "                                             cv = 10, n_jobs = -1)\n",
    "                GridSearchENT.fit(X_train, Y_train)\n",
    "                \n",
    "                # Creating a classifier that will take in the GridSearchTree \n",
    "                # as its Max Depth parameter to find it the best \n",
    "                # parameters.\n",
    "                classifier_ENT = \n",
    "                tree.DecisionTreeClassifier(criterion = 'entropy', \n",
    "                                            max_depth = GridSearchENT.best_params_['max_depth'])\n",
    "    \n",
    "                # Creating testing and training precitions based on my \n",
    "                # fitted X and Y trains.\n",
    "                classifier_ENT.fit(X_train, Y_train) \n",
    "                prediction_ENT = classifier_ENT.predict(X_test) \n",
    "                prediction_ENTtr = classifier_ENT.predict(X_train)\n",
    "                \n",
    "                # Printing the training and testing classification reports.\n",
    "                print(\"Classification report of Test DT (entropy):\", \n",
    "                      classification_report(Y_test, prediction_ENT))\n",
    "                print(\"Classification report of Train DT (entropy):\", \n",
    "                      classification_report(Y_train, prediction_ENTtr))\n",
    "                \n",
    "                # The second will be a Decision Tree with gini.\n",
    "                # Setting a list for depth length and creating a Classifier \n",
    "                # with gini.\n",
    "                Depth_list = [1, 4, 8, 16] \n",
    "                classifierGINI = tree.DecisionTreeClassifier(criterion = \n",
    "                                                             'gini') \n",
    "               \n",
    "                # Here, I will select hyperparameters via a systematic \n",
    "                # gridsearch, with a cross validation score of 10 and \n",
    "                # scoring method set to accuracy:\n",
    "                GridSearchGINI = GridSearchCV(classifierGINI, \n",
    "                                              {'max_depth':Depth_list}, \n",
    "                                              scoring = 'accuracy', \n",
    "                                              cv = 10, n_jobs = -1)\n",
    "                GridSearchGINI.fit(X_train, Y_train)\n",
    "                \n",
    "                # Creating a classifier that will take in the GridSearchTree\n",
    "                # as its Max Depth parameter to find it the best \n",
    "                # parameters.\n",
    "                classifier_GINI = \n",
    "                tree.DecisionTreeClassifier(criterion = 'entropy', \n",
    "                                            max_depth = GridSearchGINI.best_params_['max_depth'])\n",
    "                \n",
    "                # Creating testing and training precitions based on my fitted \n",
    "                # X and Y trains.\n",
    "                classifier_GINI.fit(X_train, Y_train) \n",
    "                prediction_GINI = classifier_GINI.predict(X_test) \n",
    "                prediction_GINItr = classifier_GINI.predict(X_train) \n",
    "                \n",
    "                # Printing the training and testing classification reports.\n",
    "                print(\"Classification report of Test DT (gini):\", \n",
    "                      classification_report(Y_test, prediction_GINI))\n",
    "                print(\"Classification report of Train DT (gini):\", \n",
    "                      classification_report(Y_train, prediction_GINItr))\n",
    "                \n",
    "                \n",
    "                \n",
    "            ## The second group of algorithms will be different kernels \n",
    "            # variations of the SVM algorithm.\n",
    "            if alg == 1:\n",
    "                # The first will be an SVM algorithm with a linear kernel.\n",
    "                # Creating a SVM classifier with linear kernel.\n",
    "                classifierLin = svm.SVC(kernel = 'linear') \n",
    "                \n",
    "                # I will use the same C values and gamma values as the \n",
    "                # CNM06 paper.\n",
    "                C_Values = [0.001,0.005,0.01,0.05,0.1,0.5,1,2] \n",
    "                gamma_list = [0.000001, 0.00001,0.0001, 0.001, \n",
    "                              0.01, 0.1, 1, 10, 100, 1000]\n",
    "                \n",
    "                # Creating parameter grid with C and gamma Values.\n",
    "                param_grid = {'C': [0.001,0.005,0.01,0.05,0.1,0.5,1,2], \n",
    "                              'gamma':[0.000001, 0.00001,0.0001, 0.001,\n",
    "                                       0.01, 0.1, 1, 10, 100, 1000]}\n",
    "                \n",
    "                # Here, I will select hyperparameters via a systematic \n",
    "                # gridsearch, with a cross validation score of 10 and \n",
    "                # scoring method set to accuracy:\n",
    "                GridSearchcvLin = GridSearchCV(estimator = classifierLin, \n",
    "                                               param_grid = param_grid, \n",
    "                                               scoring='accuracy', \n",
    "                                               cv = 10, n_jobs = -1)  \n",
    "                GridSearchcvLin.fit(X_train, Y_train)\n",
    "                \n",
    "                # Creating a classifier that will take in the gridsearch as \n",
    "                # its regularization parameter to find the best parameters \n",
    "                # for C.\n",
    "                classifier_svmLin = \n",
    "                svm.SVC(C = GridSearchcvLin.best_params_['C'], \n",
    "                        kernel = 'linear')\n",
    "                \n",
    "                # Creating testing and training precitions based on my fitted \n",
    "                # X and Y trains.\n",
    "                classifier_svmLin.fit(X_train, Y_train)\n",
    "                prediction_svmLin = classifier_svmLin.predict(X_test)\n",
    "                prediction_svmLintr = classifier_svmLin.predict(X_train)\n",
    "            \n",
    "                # Printing the training and testing classification reports.\n",
    "                print(\"Classification report of Test Linear SVM:\", \n",
    "                      classification_report(Y_test, prediction_svmLin))\n",
    "                print(\"Classification report of Train Linear SVM:\", \n",
    "                      classification_report(Y_train, prediction_svmLintr))\n",
    "            \n",
    "                # The second will be an SVM algorithm with a rbf kernel\n",
    "                # Creating a SVM classifier with rbf kernel.\n",
    "                classifierRbf = svm.SVC(kernel = 'rbf')\n",
    "                \n",
    "                # I will use the same C values and gamma values as the \n",
    "                # CNM06 paper.\n",
    "                C_Values = [0.001,0.005,0.01,0.05,0.1,0.5,1,2] \n",
    "                gamma_list = [0.000001, 0.00001,0.0001, 0.001, 0.01, \n",
    "                              0.1, 1, 10, 100, 1000] \n",
    "                \n",
    "                # Creating parameter grid with C and gamma Values.\n",
    "                param_grid = {'C': [0.001,0.005,0.01,0.05,0.1,0.5,1,2], \n",
    "                              'gamma':[0.000001, 0.00001,0.0001, 0.001, \n",
    "                                       0.01, 0.1, 1, 10, 100, 1000]}\n",
    "                \n",
    "                # Here, I will select hyperparameters via a systematic \n",
    "                # gridsearch, with a cross validation score of 10 and \n",
    "                # scoring method set to accuracy:\n",
    "                GridSearchcvRBF = GridSearchCV(estimator = classifierRbf, \n",
    "                                               param_grid = param_grid, \n",
    "                                               scoring='accuracy', \n",
    "                                               cv = 10, n_jobs = -1)  \n",
    "                GridSearchcvRBF.fit(X_train, Y_train)\n",
    "                \n",
    "                # Creating a classifier that will take in the gridsearch as \n",
    "                # its regularization parameter to find the best parameters \n",
    "                # for C.\n",
    "                classifier_svmRBF = \n",
    "                svm.SVC(C = GridSearchcvRBF.best_params_['C'], \n",
    "                        kernel = 'rbf')\n",
    "                \n",
    "                # Creating testing and training precitions based on my fitted \n",
    "                # X and Y trains.\n",
    "                classifier_svmRBF.fit(X_train, Y_train)\n",
    "                prediction_svmRBF = classifier_svmRBF.predict(X_test)\n",
    "                prediction_svmRBFtr = classifier_svmRBF.predict(X_train)\n",
    "                \n",
    "                # Printing the training and testing classification reports.\n",
    "                print(\"Classification report of Test RBF SVM:\", \n",
    "                      classification_report(Y_test, prediction_svmRBF))\n",
    "                print(\"Classification report of Train RBF SVM:\", \n",
    "                      classification_report(Y_train, prediction_svmRBFtr))\n",
    "            \n",
    "                # The last one will be an SVM algorithm with a sigmoid kernel.\n",
    "                # Creating a SVM classifier with a sigmoid kernel.\n",
    "                classifierSig = svm.SVC(kernel = 'sigmoid') \n",
    "                \n",
    "                # I will use the same C values and gamma values as the \n",
    "                # CNM06 paper.\n",
    "                C_Values = [0.001,0.005,0.01,0.05,0.1,0.5,1,2] \n",
    "                gamma_list = [0.000001, 0.00001,0.0001, 0.001, 0.01, \n",
    "                              0.1, 1, 10, 100, 1000] \n",
    "                \n",
    "                # Creating parameter grid with C and gamma Values\n",
    "                param_grid = {'C': [0.001,0.005,0.01,0.05,0.1,0.5,1,2], \n",
    "                              'gamma':[0.000001, 0.00001,0.0001, 0.001, \n",
    "                                       0.01, 0.1, 1, 10, 100, 1000]}\n",
    "                \n",
    "                # Here, I will select hyperparameters via a systematic \n",
    "                # gridsearch, with a cross validation score of 10 and \n",
    "                # scoring method set to accuracy:\n",
    "                GridSearchcvSIG = GridSearchCV(estimator = classifierSig, \n",
    "                                               param_grid = param_grid, \n",
    "                                               scoring='accuracy', \n",
    "                                               cv = 10, n_jobs = -1)  \n",
    "                GridSearchcvSIG.fit(X_train, Y_train)\n",
    "                \n",
    "                # Creating a classifier that will take in the gridsearch as \n",
    "                # its regularization parameter to find the best parameters \n",
    "                # for C.\n",
    "                classifier_svmSIG = svm.SVC(C = GridSearchcvSIG.best_params_['C'], kernel = 'sigmoid')\n",
    "                \n",
    "                # Creating testing and training precitions based on my fitted \n",
    "                # X and Y trains.\n",
    "                classifier_svmSIG.fit(X_train, Y_train)\n",
    "                prediction_svmSig = classifier_svmSIG.predict(X_test)\n",
    "                prediction_svmSigtr = classifier_svmSIG.predict(X_train)\n",
    "                \n",
    "                # Printing the training and testing classification reports.\n",
    "                print(\"Classification report of Test Sigmoid SVM:\", \n",
    "                      classification_report(Y_test, prediction_svmSig))\n",
    "                print(\"Classification report of Train Sigmoid SVM:\", \n",
    "                      classification_report(Y_train, prediction_svmSigtr))\n",
    "                \n",
    "                \n",
    "                \n",
    "            ## The third group of algorithms will be Random Forests.\n",
    "            if alg == 2:\n",
    "                # The first will be a Random forest with Gini criterion.\n",
    "                # Setting same depth lengths and estimators as the \n",
    "                # CNM06 paper.\n",
    "                D_list = [1, 2, 4, 6, 8, 12, 16, 20] \n",
    "                n_estimators = 1024\n",
    "                \n",
    "                # Creating a Random Forest classifier with 1024 estimators \n",
    "                # and an gini criterion.\n",
    "                classifierRFgini = \n",
    "                RandomForestClassifier(n_estimators = n_estimators, \n",
    "                                       criterion = 'gini')\n",
    "                \n",
    "                # Here, I will select hyperparameters via a systematic \n",
    "                # gridsearch, with a cross validation score of 10 and \n",
    "                # scoring method set to accuracy:\n",
    "                GridSearchRFgini = GridSearchCV(classifierRFgini, \n",
    "                                                {'max_depth':D_list}, \n",
    "                                                scoring = 'accuracy', \n",
    "                                                cv = 10, n_jobs = -1)\n",
    "                GridSearchRFgini.fit(X_train, Y_train)\n",
    "                \n",
    "                # Creating a classifier that will take in the gridsearch as \n",
    "                # its regularization parameter to find the best parameters \n",
    "                # for C.\n",
    "                classifier_RFgini = \n",
    "                RandomForestClassifier(criterion = 'entropy',\n",
    "                                       max_depth = GridSearchRFgini.best_params_['max_depth'])\n",
    "                \n",
    "                # Creating testing and training precitions based on my fitted \n",
    "                # X and Y trains.\n",
    "                classifier_RFgini.fit(X_train, Y_train)\n",
    "                prediction_RFgini = classifier_RFgini.predict(X_test)\n",
    "                prediction_RFginitr = classifier_RFgini.predict(X_train)\n",
    "                \n",
    "                # Printing the training and testing classification reports.\n",
    "                print(\"Classification report of Test RF (gini):\", \n",
    "                      classification_report(Y_test, prediction_RFgini))\n",
    "                print(\"Classification report of Train RF (gini):\", \n",
    "                      classification_report(Y_train, prediction_RFginitr))\n",
    "            \n",
    "                #The second will be a Random forest with an Entropy criterion.\n",
    "                # Setting same depth lengths and estimators as the \n",
    "                # CNM06 paper.\n",
    "                D_list = [1, 2, 4, 6, 8, 12, 16, 20] \n",
    "                n_estimators = 1024\n",
    "                \n",
    "                # Creating a Random Forest classifier with 1024 estimators \n",
    "                # and an entropy criterion.\n",
    "                classifierRFent = \n",
    "                RandomForestClassifier(n_estimators = n_estimators, \n",
    "                                       criterion = 'entropy')\n",
    "                \n",
    "                # Here, I will select hyperparameters via a systematic \n",
    "                # gridsearch, with a cross validation score of 10 and \n",
    "                # scoring method set to accuracy:\n",
    "                GridSearchRFent = \n",
    "                GridSearchCV(classifierRFent, \n",
    "                             {'max_depth':D_list}, \n",
    "                             scoring = 'accuracy', \n",
    "                             cv = 10, n_jobs = -1)\n",
    "                GridSearchRFent.fit(X_train, Y_train)\n",
    "                \n",
    "                # Creating a classifier that will take in the gridsearch \n",
    "                # as its regularization parameter to find the best parameters \n",
    "                # for C.\n",
    "                classifier_RFent = \n",
    "                RandomForestClassifier(criterion = 'entropy',\n",
    "                                       max_depth = GridSearchRFent.best_params_['max_depth'])\n",
    "                \n",
    "                # Creating testing and training precitions based on my fitted \n",
    "                # X and Y trains.\n",
    "                classifier_RFent.fit(X_train, Y_train)\n",
    "                prediction_RFent = classifier_RFent.predict(X_test)\n",
    "                prediction_RFenttr = classifier_RFent.predict(X_train)\n",
    "                \n",
    "                # Printing the training and testing classification reports.\n",
    "                print(\"Classification report of Test RF (entropy):\", \n",
    "                      classification_report(Y_test, prediction_RFent))\n",
    "                print(\"Classification report of Train RF (entropy):\", \n",
    "                      classification_report(Y_train, prediction_RFenttr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Skin Tone Dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following code in this block is for my Skin Dataset:\n",
    "\n",
    "# We begin by making a massive for-loop to go through 10 trials.\n",
    "for iterations in range(10):\n",
    "    \n",
    "    # Shuffling the dataset.\n",
    "    skinData = shuffle(skinData) \n",
    "    \n",
    "    # Setting up X values to be all variables except the Y column, \n",
    "    # and Y values to be the Y column. \n",
    "    X = skinData.drop('Y', axis = 1)  \n",
    "    Y = skinData['Y'] \n",
    "    \n",
    "    # Printing the number of each iteration as we go up. \n",
    "    print(\"Trial Number:\", iterations + 1) \n",
    "\n",
    "\n",
    "    # Setting up a for loop within my iterations loop, which will split my \n",
    "    # data 4 times, in different ways.\n",
    "    for split in range(4):\n",
    "        if split == 0:\n",
    "            # For the first split, I will do a 80(training)/20(testing) \n",
    "            # split, with a Random State of 50.\n",
    "            X_train, X_test, Y_train, Y_test = \n",
    "            train_test_split(X, Y, test_size = 0.2, random_state = 50)\n",
    "            \n",
    "            # Standardizing features by removing the mean and scaling to a \n",
    "            # unit variance.\n",
    "            stsc = StandardScaler() \n",
    "            \n",
    "            # Transforming X train and X test based on the \n",
    "            # standardization above.\n",
    "            X_train = stsc.fit_transform(X_train)\n",
    "            X_test = stsc.fit_transform(X_test)\n",
    "            print(\"Data Split 80/20:\")\n",
    "        \n",
    "        if split == 1:\n",
    "            # For the second split, I will do a 70(training)/30(testing)\n",
    "            # split, with a Random State of 50.\n",
    "            X_train, X_test, Y_train, Y_test = \n",
    "            train_test_split(X, Y, test_size = 0.3, random_state = 50)\n",
    "            \n",
    "            # Standardizing features by removing the mean and scaling to a \n",
    "            # unit variance.\n",
    "            stsc = StandardScaler() \n",
    "            \n",
    "            # Transforming X train and X test based on the \n",
    "            # standardization above.\n",
    "            X_train = stsc.fit_transform(X_train) \n",
    "            X_test = stsc.fit_transform(X_test)\n",
    "            print(\"Data Split 70/30:\")\n",
    "\n",
    "        if split == 2:\n",
    "            # For the third split, I will do a 60(training)/40(testing) \n",
    "            # split, with a Random State of 50.\n",
    "            X_train, X_test, Y_train, Y_test = \n",
    "            train_test_split(X, Y, test_size = 0.4, random_state = 50)\n",
    "            \n",
    "            # Standardizing features by removing the mean and scaling to a \n",
    "            # unit variance.\n",
    "            stsc = StandardScaler() \n",
    "            \n",
    "            # Transforming X train and X test based on the \n",
    "            # standardization above.\n",
    "            X_train = stsc.fit_transform(X_train) \n",
    "            X_test = stsc.fit_transform(X_test)\n",
    "            print(\"Data Split 60/40:\")\n",
    "            \n",
    "        if split == 3:\n",
    "            # For the fourth split, I will do a 50(training)/50(testing) \n",
    "            # split, with a Random State of 50.\n",
    "            X_train, X_test, Y_train, Y_test = \n",
    "            train_test_split(X, Y, test_size = 0.5, random_state = 50)\n",
    "            \n",
    "            # Standardizing features by removing the mean and scaling to a \n",
    "            # unit variance.\n",
    "            stsc = StandardScaler()\n",
    "        \n",
    "            # Transforming X train and X test based on the \n",
    "            # standardization above.\n",
    "            X_train = stsc.fit_transform(X_train) \n",
    "            X_test = stsc.fit_transform(X_test) \n",
    "            print(\"Data Split 50/50:\")\n",
    "\n",
    "            \n",
    "        # Setting up a for-loop for my 3 algorithms. \n",
    "        for alg in range(3):\n",
    "            ## The first group of algorithms will be Decision Trees .\n",
    "            if alg == 0:\n",
    "                # The first will be a Decision Tree with entropy.\n",
    "                # Setting a list for depth length and creating a Classifier \n",
    "                # with entropy.\n",
    "                Depth_list = [1, 4, 8, 16] \n",
    "                classifierENT = tree.DecisionTreeClassifier(criterion = \n",
    "                                                            'entropy') \n",
    "                \n",
    "                # Here, I will select hyperparameters via a systematic \n",
    "                # gridsearch, with a cross validation score of 10 and \n",
    "                # scoring method set to accuracy:\n",
    "                GridSearchENT = GridSearchCV(classifierENT, \n",
    "                                             {'max_depth':Depth_list},\n",
    "                                             scoring = 'accuracy', \n",
    "                                             cv = 10, n_jobs = -1)\n",
    "                GridSearchENT.fit(X_train, Y_train)\n",
    "                \n",
    "                # Creating a classifier that will take in the GridSearchTree \n",
    "                # as its Max Depth parameter to find it the best \n",
    "                # parameters.\n",
    "                classifier_ENT = \n",
    "                tree.DecisionTreeClassifier(criterion = 'entropy', \n",
    "                                            max_depth = GridSearchENT.best_params_['max_depth'])\n",
    "    \n",
    "                # Creating testing and training precitions based on my \n",
    "                # fitted X and Y trains.\n",
    "                classifier_ENT.fit(X_train, Y_train) \n",
    "                prediction_ENT = classifier_ENT.predict(X_test) \n",
    "                prediction_ENTtr = classifier_ENT.predict(X_train)\n",
    "                \n",
    "                # Printing the training and testing classification reports.\n",
    "                print(\"Classification report of Test DT (entropy):\", \n",
    "                      classification_report(Y_test, prediction_ENT))\n",
    "                print(\"Classification report of Train DT (entropy):\", \n",
    "                      classification_report(Y_train, prediction_ENTtr))\n",
    "                \n",
    "                # The second will be a Decision Tree with gini.\n",
    "                # Setting a list for depth length and creating a Classifier \n",
    "                # with gini.\n",
    "                Depth_list = [1, 4, 8, 16] \n",
    "                classifierGINI = tree.DecisionTreeClassifier(criterion = \n",
    "                                                             'gini') \n",
    "               \n",
    "                # Here, I will select hyperparameters via a systematic \n",
    "                # gridsearch, with a cross validation score of 10 and \n",
    "                # scoring method set to accuracy:\n",
    "                GridSearchGINI = GridSearchCV(classifierGINI, \n",
    "                                              {'max_depth':Depth_list}, \n",
    "                                              scoring = 'accuracy', \n",
    "                                              cv = 10, n_jobs = -1)\n",
    "                GridSearchGINI.fit(X_train, Y_train)\n",
    "                \n",
    "                # Creating a classifier that will take in the GridSearchTree\n",
    "                # as its Max Depth parameter to find it the best \n",
    "                # parameters.\n",
    "                classifier_GINI = \n",
    "                tree.DecisionTreeClassifier(criterion = 'entropy', \n",
    "                                            max_depth = GridSearchGINI.best_params_['max_depth'])\n",
    "                \n",
    "                # Creating testing and training precitions based on my fitted \n",
    "                # X and Y trains.\n",
    "                classifier_GINI.fit(X_train, Y_train) \n",
    "                prediction_GINI = classifier_GINI.predict(X_test) \n",
    "                prediction_GINItr = classifier_GINI.predict(X_train) \n",
    "                \n",
    "                # Printing the training and testing classification reports.\n",
    "                print(\"Classification report of Test DT (gini):\", \n",
    "                      classification_report(Y_test, prediction_GINI))\n",
    "                print(\"Classification report of Train DT (gini):\", \n",
    "                      classification_report(Y_train, prediction_GINItr))\n",
    "                \n",
    "                \n",
    "                \n",
    "            ## The second group of algorithms will be different kernels \n",
    "            # variations of the SVM algorithm.\n",
    "            if alg == 1:\n",
    "                # The first will be an SVM algorithm with a linear kernel.\n",
    "                # Creating a SVM classifier with linear kernel.\n",
    "                classifierLin = svm.SVC(kernel = 'linear') \n",
    "                \n",
    "                # I will use the same C values and gamma values as the \n",
    "                # CNM06 paper.\n",
    "                C_Values = [0.001,0.005,0.01,0.05,0.1,0.5,1,2] \n",
    "                gamma_list = [0.000001, 0.00001,0.0001, 0.001, \n",
    "                              0.01, 0.1, 1, 10, 100, 1000]\n",
    "                \n",
    "                # Creating parameter grid with C and gamma Values.\n",
    "                param_grid = {'C': [0.001,0.005,0.01,0.05,0.1,0.5,1,2], \n",
    "                              'gamma':[0.000001, 0.00001,0.0001, 0.001,\n",
    "                                       0.01, 0.1, 1, 10, 100, 1000]}\n",
    "                \n",
    "                # Here, I will select hyperparameters via a systematic \n",
    "                # gridsearch, with a cross validation score of 10 and \n",
    "                # scoring method set to accuracy:\n",
    "                GridSearchcvLin = GridSearchCV(estimator = classifierLin, \n",
    "                                               param_grid = param_grid, \n",
    "                                               scoring='accuracy', \n",
    "                                               cv = 10, n_jobs = -1)  \n",
    "                GridSearchcvLin.fit(X_train, Y_train)\n",
    "                \n",
    "                # Creating a classifier that will take in the gridsearch as \n",
    "                # its regularization parameter to find the best parameters \n",
    "                # for C.\n",
    "                classifier_svmLin = \n",
    "                svm.SVC(C = GridSearchcvLin.best_params_['C'], \n",
    "                        kernel = 'linear')\n",
    "                \n",
    "                # Creating testing and training precitions based on my fitted \n",
    "                # X and Y trains.\n",
    "                classifier_svmLin.fit(X_train, Y_train)\n",
    "                prediction_svmLin = classifier_svmLin.predict(X_test)\n",
    "                prediction_svmLintr = classifier_svmLin.predict(X_train)\n",
    "            \n",
    "                # Printing the training and testing classification reports.\n",
    "                print(\"Classification report of Test Linear SVM:\", \n",
    "                      classification_report(Y_test, prediction_svmLin))\n",
    "                print(\"Classification report of Train Linear SVM:\", \n",
    "                      classification_report(Y_train, prediction_svmLintr))\n",
    "            \n",
    "                # The second will be an SVM algorithm with a rbf kernel\n",
    "                # Creating a SVM classifier with rbf kernel.\n",
    "                classifierRbf = svm.SVC(kernel = 'rbf')\n",
    "                \n",
    "                # I will use the same C values and gamma values as the \n",
    "                # CNM06 paper.\n",
    "                C_Values = [0.001,0.005,0.01,0.05,0.1,0.5,1,2] \n",
    "                gamma_list = [0.000001, 0.00001,0.0001, 0.001, 0.01, \n",
    "                              0.1, 1, 10, 100, 1000] \n",
    "                \n",
    "                # Creating parameter grid with C and gamma Values.\n",
    "                param_grid = {'C': [0.001,0.005,0.01,0.05,0.1,0.5,1,2], \n",
    "                              'gamma':[0.000001, 0.00001,0.0001, 0.001, \n",
    "                                       0.01, 0.1, 1, 10, 100, 1000]}\n",
    "                \n",
    "                # Here, I will select hyperparameters via a systematic \n",
    "                # gridsearch, with a cross validation score of 10 and \n",
    "                # scoring method set to accuracy:\n",
    "                GridSearchcvRBF = GridSearchCV(estimator = classifierRbf, \n",
    "                                               param_grid = param_grid, \n",
    "                                               scoring='accuracy', \n",
    "                                               cv = 10, n_jobs = -1)  \n",
    "                GridSearchcvRBF.fit(X_train, Y_train)\n",
    "                \n",
    "                # Creating a classifier that will take in the gridsearch as \n",
    "                # its regularization parameter to find the best parameters \n",
    "                # for C.\n",
    "                classifier_svmRBF = \n",
    "                svm.SVC(C = GridSearchcvRBF.best_params_['C'], \n",
    "                        kernel = 'rbf')\n",
    "                \n",
    "                # Creating testing and training precitions based on my fitted \n",
    "                # X and Y trains.\n",
    "                classifier_svmRBF.fit(X_train, Y_train)\n",
    "                prediction_svmRBF = classifier_svmRBF.predict(X_test)\n",
    "                prediction_svmRBFtr = classifier_svmRBF.predict(X_train)\n",
    "                \n",
    "                # Printing the training and testing classification reports.\n",
    "                print(\"Classification report of Test RBF SVM:\", \n",
    "                      classification_report(Y_test, prediction_svmRBF))\n",
    "                print(\"Classification report of Train RBF SVM:\", \n",
    "                      classification_report(Y_train, prediction_svmRBFtr))\n",
    "            \n",
    "                # The last one will be an SVM algorithm with a sigmoid kernel.\n",
    "                # Creating a SVM classifier with a sigmoid kernel.\n",
    "                classifierSig = svm.SVC(kernel = 'sigmoid') \n",
    "                \n",
    "                # I will use the same C values and gamma values as the \n",
    "                # CNM06 paper.\n",
    "                C_Values = [0.001,0.005,0.01,0.05,0.1,0.5,1,2] \n",
    "                gamma_list = [0.000001, 0.00001,0.0001, 0.001, 0.01, \n",
    "                              0.1, 1, 10, 100, 1000] \n",
    "                \n",
    "                # Creating parameter grid with C and gamma Values\n",
    "                param_grid = {'C': [0.001,0.005,0.01,0.05,0.1,0.5,1,2], \n",
    "                              'gamma':[0.000001, 0.00001,0.0001, 0.001, \n",
    "                                       0.01, 0.1, 1, 10, 100, 1000]}\n",
    "                \n",
    "                # Here, I will select hyperparameters via a systematic \n",
    "                # gridsearch, with a cross validation score of 10 and \n",
    "                # scoring method set to accuracy:\n",
    "                GridSearchcvSIG = GridSearchCV(estimator = classifierSig, \n",
    "                                               param_grid = param_grid, \n",
    "                                               scoring='accuracy', \n",
    "                                               cv = 10, n_jobs = -1)  \n",
    "                GridSearchcvSIG.fit(X_train, Y_train)\n",
    "                \n",
    "                # Creating a classifier that will take in the gridsearch as \n",
    "                # its regularization parameter to find the best parameters \n",
    "                # for C.\n",
    "                classifier_svmSIG = svm.SVC(C = GridSearchcvSIG.best_params_['C'], kernel = 'sigmoid')\n",
    "                \n",
    "                # Creating testing and training precitions based on my fitted \n",
    "                # X and Y trains.\n",
    "                classifier_svmSIG.fit(X_train, Y_train)\n",
    "                prediction_svmSig = classifier_svmSIG.predict(X_test)\n",
    "                prediction_svmSigtr = classifier_svmSIG.predict(X_train)\n",
    "                \n",
    "                # Printing the training and testing classification reports.\n",
    "                print(\"Classification report of Test Sigmoid SVM:\", \n",
    "                      classification_report(Y_test, prediction_svmSig))\n",
    "                print(\"Classification report of Train Sigmoid SVM:\", \n",
    "                      classification_report(Y_train, prediction_svmSigtr))\n",
    "                \n",
    "                \n",
    "                \n",
    "            ## The third group of algorithms will be Random Forests.\n",
    "            if alg == 2:\n",
    "                # The first will be a Random forest with Gini criterion.\n",
    "                # Setting same depth lengths and estimators as the \n",
    "                # CNM06 paper.\n",
    "                D_list = [1, 2, 4, 6, 8, 12, 16, 20] \n",
    "                n_estimators = 1024\n",
    "                \n",
    "                # Creating a Random Forest classifier with 1024 estimators \n",
    "                # and an gini criterion.\n",
    "                classifierRFgini = \n",
    "                RandomForestClassifier(n_estimators = n_estimators, \n",
    "                                       criterion = 'gini')\n",
    "                \n",
    "                # Here, I will select hyperparameters via a systematic \n",
    "                # gridsearch, with a cross validation score of 10 and \n",
    "                # scoring method set to accuracy:\n",
    "                GridSearchRFgini = GridSearchCV(classifierRFgini, \n",
    "                                                {'max_depth':D_list}, \n",
    "                                                scoring = 'accuracy', \n",
    "                                                cv = 10, n_jobs = -1)\n",
    "                GridSearchRFgini.fit(X_train, Y_train)\n",
    "                \n",
    "                # Creating a classifier that will take in the gridsearch as \n",
    "                # its regularization parameter to find the best parameters \n",
    "                # for C.\n",
    "                classifier_RFgini = \n",
    "                RandomForestClassifier(criterion = 'entropy',\n",
    "                                       max_depth = GridSearchRFgini.best_params_['max_depth'])\n",
    "                \n",
    "                # Creating testing and training precitions based on my fitted \n",
    "                # X and Y trains.\n",
    "                classifier_RFgini.fit(X_train, Y_train)\n",
    "                prediction_RFgini = classifier_RFgini.predict(X_test)\n",
    "                prediction_RFginitr = classifier_RFgini.predict(X_train)\n",
    "                \n",
    "                # Printing the training and testing classification reports.\n",
    "                print(\"Classification report of Test RF (gini):\", \n",
    "                      classification_report(Y_test, prediction_RFgini))\n",
    "                print(\"Classification report of Train RF (gini):\", \n",
    "                      classification_report(Y_train, prediction_RFginitr))\n",
    "            \n",
    "                #The second will be a Random forest with an Entropy criterion.\n",
    "                # Setting same depth lengths and estimators as the \n",
    "                # CNM06 paper.\n",
    "                D_list = [1, 2, 4, 6, 8, 12, 16, 20] \n",
    "                n_estimators = 1024\n",
    "                \n",
    "                # Creating a Random Forest classifier with 1024 estimators \n",
    "                # and an entropy criterion.\n",
    "                classifierRFent = \n",
    "                RandomForestClassifier(n_estimators = n_estimators, \n",
    "                                       criterion = 'entropy')\n",
    "                \n",
    "                # Here, I will select hyperparameters via a systematic \n",
    "                # gridsearch, with a cross validation score of 10 and \n",
    "                # scoring method set to accuracy:\n",
    "                GridSearchRFent = \n",
    "                GridSearchCV(classifierRFent, \n",
    "                             {'max_depth':D_list}, \n",
    "                             scoring = 'accuracy', \n",
    "                             cv = 10, n_jobs = -1)\n",
    "                GridSearchRFent.fit(X_train, Y_train)\n",
    "                \n",
    "                # Creating a classifier that will take in the gridsearch \n",
    "                # as its regularization parameter to find the best parameters \n",
    "                # for C.\n",
    "                classifier_RFent = \n",
    "                RandomForestClassifier(criterion = 'entropy',\n",
    "                                       max_depth = GridSearchRFent.best_params_['max_depth'])\n",
    "                \n",
    "                # Creating testing and training precitions based on my fitted \n",
    "                # X and Y trains.\n",
    "                classifier_RFent.fit(X_train, Y_train)\n",
    "                prediction_RFent = classifier_RFent.predict(X_test)\n",
    "                prediction_RFenttr = classifier_RFent.predict(X_train)\n",
    "                \n",
    "                # Printing the training and testing classification reports.\n",
    "                print(\"Classification report of Test RF (entropy):\", \n",
    "                      classification_report(Y_test, prediction_RFent))\n",
    "                print(\"Classification report of Train RF (entropy):\", \n",
    "                      classification_report(Y_train, prediction_RFenttr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adult Dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following code in this block is for my Adult Dataset:\n",
    "\n",
    "# We begin by making a massive for-loop to go through 10 trials.\n",
    "for iterations in range(10):\n",
    "    \n",
    "    # Shuffling the dataset.\n",
    "    adultData = shuffle(adultData) \n",
    "    \n",
    "    # Assiigning X values to be all columns except salary, \n",
    "    # and Y values to be salary.\n",
    "    X = adultData.drop('salary', axis = 1)\n",
    "    Y = adultData['salary']\n",
    "    \n",
    "    # Printing the number of each iteration as we go up. \n",
    "    print(\"Trial Number:\", iterations + 1)  \n",
    "\n",
    "\n",
    "    # Setting up a for loop within my iterations loop, which will split my \n",
    "    # data 4 times, in different ways.\n",
    "    for split in range(4):\n",
    "        if split == 0:\n",
    "            # For the first split, I will do a 80(training)/20(testing) \n",
    "            # split, with a Random State of 50.\n",
    "            X_train, X_test, Y_train, Y_test = \n",
    "            train_test_split(X, Y, test_size = 0.2, random_state = 50)\n",
    "            \n",
    "            # Standardizing features by removing the mean and scaling to a \n",
    "            # unit variance.\n",
    "            stsc = StandardScaler() \n",
    "            \n",
    "            # Transforming X train and X test based on the \n",
    "            # standardization above.\n",
    "            X_train = stsc.fit_transform(X_train)\n",
    "            X_test = stsc.fit_transform(X_test)\n",
    "            print(\"Data Split 80/20:\")\n",
    "        \n",
    "        if split == 1:\n",
    "            # For the second split, I will do a 70(training)/30(testing)\n",
    "            # split, with a Random State of 50.\n",
    "            X_train, X_test, Y_train, Y_test = \n",
    "            train_test_split(X, Y, test_size = 0.3, random_state = 50)\n",
    "            \n",
    "            # Standardizing features by removing the mean and scaling to a \n",
    "            # unit variance.\n",
    "            stsc = StandardScaler() \n",
    "            \n",
    "            # Transforming X train and X test based on the \n",
    "            # standardization above.\n",
    "            X_train = stsc.fit_transform(X_train) \n",
    "            X_test = stsc.fit_transform(X_test)\n",
    "            print(\"Data Split 70/30:\")\n",
    "\n",
    "        if split == 2:\n",
    "            # For the third split, I will do a 60(training)/40(testing) \n",
    "            # split, with a Random State of 50.\n",
    "            X_train, X_test, Y_train, Y_test = \n",
    "            train_test_split(X, Y, test_size = 0.4, random_state = 50)\n",
    "            \n",
    "            # Standardizing features by removing the mean and scaling to a \n",
    "            # unit variance.\n",
    "            stsc = StandardScaler() \n",
    "            \n",
    "            # Transforming X train and X test based on the \n",
    "            # standardization above.\n",
    "            X_train = stsc.fit_transform(X_train) \n",
    "            X_test = stsc.fit_transform(X_test)\n",
    "            print(\"Data Split 60/40:\")\n",
    "            \n",
    "        if split == 3:\n",
    "            # For the fourth split, I will do a 50(training)/50(testing) \n",
    "            # split, with a Random State of 50.\n",
    "            X_train, X_test, Y_train, Y_test = \n",
    "            train_test_split(X, Y, test_size = 0.5, random_state = 50)\n",
    "            \n",
    "            # Standardizing features by removing the mean and scaling to a \n",
    "            # unit variance.\n",
    "            stsc = StandardScaler()\n",
    "        \n",
    "            # Transforming X train and X test based on the \n",
    "            # standardization above.\n",
    "            X_train = stsc.fit_transform(X_train) \n",
    "            X_test = stsc.fit_transform(X_test) \n",
    "            print(\"Data Split 50/50:\")\n",
    "\n",
    "            \n",
    "        # Setting up a for-loop for my 3 algorithms. \n",
    "        for alg in range(3):\n",
    "            ## The first group of algorithms will be Decision Trees .\n",
    "            if alg == 0:\n",
    "                # The first will be a Decision Tree with entropy.\n",
    "                # Setting a list for depth length and creating a Classifier \n",
    "                # with entropy.\n",
    "                Depth_list = [1, 4, 8, 16] \n",
    "                classifierENT = tree.DecisionTreeClassifier(criterion = \n",
    "                                                            'entropy') \n",
    "                \n",
    "                # Here, I will select hyperparameters via a systematic \n",
    "                # gridsearch, with a cross validation score of 10 and \n",
    "                # scoring method set to accuracy:\n",
    "                GridSearchENT = GridSearchCV(classifierENT, \n",
    "                                             {'max_depth':Depth_list},\n",
    "                                             scoring = 'accuracy', \n",
    "                                             cv = 10, n_jobs = -1)\n",
    "                GridSearchENT.fit(X_train, Y_train)\n",
    "                \n",
    "                # Creating a classifier that will take in the GridSearchTree \n",
    "                # as its Max Depth parameter to find it the best \n",
    "                # parameters.\n",
    "                classifier_ENT = \n",
    "                tree.DecisionTreeClassifier(criterion = 'entropy', \n",
    "                                            max_depth = GridSearchENT.best_params_['max_depth'])\n",
    "    \n",
    "                # Creating testing and training precitions based on my \n",
    "                # fitted X and Y trains.\n",
    "                classifier_ENT.fit(X_train, Y_train) \n",
    "                prediction_ENT = classifier_ENT.predict(X_test) \n",
    "                prediction_ENTtr = classifier_ENT.predict(X_train)\n",
    "                \n",
    "                # Printing the training and testing classification reports.\n",
    "                print(\"Classification report of Test DT (entropy):\", \n",
    "                      classification_report(Y_test, prediction_ENT))\n",
    "                print(\"Classification report of Train DT (entropy):\", \n",
    "                      classification_report(Y_train, prediction_ENTtr))\n",
    "                \n",
    "                # The second will be a Decision Tree with gini.\n",
    "                # Setting a list for depth length and creating a Classifier \n",
    "                # with gini.\n",
    "                Depth_list = [1, 4, 8, 16] \n",
    "                classifierGINI = tree.DecisionTreeClassifier(criterion = \n",
    "                                                             'gini') \n",
    "               \n",
    "                # Here, I will select hyperparameters via a systematic \n",
    "                # gridsearch, with a cross validation score of 10 and \n",
    "                # scoring method set to accuracy:\n",
    "                GridSearchGINI = GridSearchCV(classifierGINI, \n",
    "                                              {'max_depth':Depth_list}, \n",
    "                                              scoring = 'accuracy', \n",
    "                                              cv = 10, n_jobs = -1)\n",
    "                GridSearchGINI.fit(X_train, Y_train)\n",
    "                \n",
    "                # Creating a classifier that will take in the GridSearchTree\n",
    "                # as its Max Depth parameter to find it the best \n",
    "                # parameters.\n",
    "                classifier_GINI = \n",
    "                tree.DecisionTreeClassifier(criterion = 'entropy', \n",
    "                                            max_depth = GridSearchGINI.best_params_['max_depth'])\n",
    "                \n",
    "                # Creating testing and training precitions based on my fitted \n",
    "                # X and Y trains.\n",
    "                classifier_GINI.fit(X_train, Y_train) \n",
    "                prediction_GINI = classifier_GINI.predict(X_test) \n",
    "                prediction_GINItr = classifier_GINI.predict(X_train) \n",
    "                \n",
    "                # Printing the training and testing classification reports.\n",
    "                print(\"Classification report of Test DT (gini):\", \n",
    "                      classification_report(Y_test, prediction_GINI))\n",
    "                print(\"Classification report of Train DT (gini):\", \n",
    "                      classification_report(Y_train, prediction_GINItr))\n",
    "                \n",
    "                \n",
    "                \n",
    "            ## The second group of algorithms will be different kernels \n",
    "            # variations of the SVM algorithm.\n",
    "            if alg == 1:\n",
    "                # The first will be an SVM algorithm with a linear kernel.\n",
    "                # Creating a SVM classifier with linear kernel.\n",
    "                classifierLin = svm.SVC(kernel = 'linear') \n",
    "                \n",
    "                # I will use the same C values and gamma values as the \n",
    "                # CNM06 paper.\n",
    "                C_Values = [0.001,0.005,0.01,0.05,0.1,0.5,1,2] \n",
    "                gamma_list = [0.000001, 0.00001,0.0001, 0.001, \n",
    "                              0.01, 0.1, 1, 10, 100, 1000]\n",
    "                \n",
    "                # Creating parameter grid with C and gamma Values.\n",
    "                param_grid = {'C': [0.001,0.005,0.01,0.05,0.1,0.5,1,2], \n",
    "                              'gamma':[0.000001, 0.00001,0.0001, 0.001,\n",
    "                                       0.01, 0.1, 1, 10, 100, 1000]}\n",
    "                \n",
    "                # Here, I will select hyperparameters via a systematic \n",
    "                # gridsearch, with a cross validation score of 10 and \n",
    "                # scoring method set to accuracy:\n",
    "                GridSearchcvLin = GridSearchCV(estimator = classifierLin, \n",
    "                                               param_grid = param_grid, \n",
    "                                               scoring='accuracy', \n",
    "                                               cv = 10, n_jobs = -1)  \n",
    "                GridSearchcvLin.fit(X_train, Y_train)\n",
    "                \n",
    "                # Creating a classifier that will take in the gridsearch as \n",
    "                # its regularization parameter to find the best parameters \n",
    "                # for C.\n",
    "                classifier_svmLin = \n",
    "                svm.SVC(C = GridSearchcvLin.best_params_['C'], \n",
    "                        kernel = 'linear')\n",
    "                \n",
    "                # Creating testing and training precitions based on my fitted \n",
    "                # X and Y trains.\n",
    "                classifier_svmLin.fit(X_train, Y_train)\n",
    "                prediction_svmLin = classifier_svmLin.predict(X_test)\n",
    "                prediction_svmLintr = classifier_svmLin.predict(X_train)\n",
    "            \n",
    "                # Printing the training and testing classification reports.\n",
    "                print(\"Classification report of Test Linear SVM:\", \n",
    "                      classification_report(Y_test, prediction_svmLin))\n",
    "                print(\"Classification report of Train Linear SVM:\", \n",
    "                      classification_report(Y_train, prediction_svmLintr))\n",
    "            \n",
    "                # The second will be an SVM algorithm with a rbf kernel\n",
    "                # Creating a SVM classifier with rbf kernel.\n",
    "                classifierRbf = svm.SVC(kernel = 'rbf')\n",
    "                \n",
    "                # I will use the same C values and gamma values as the \n",
    "                # CNM06 paper.\n",
    "                C_Values = [0.001,0.005,0.01,0.05,0.1,0.5,1,2] \n",
    "                gamma_list = [0.000001, 0.00001,0.0001, 0.001, 0.01, \n",
    "                              0.1, 1, 10, 100, 1000] \n",
    "                \n",
    "                # Creating parameter grid with C and gamma Values.\n",
    "                param_grid = {'C': [0.001,0.005,0.01,0.05,0.1,0.5,1,2], \n",
    "                              'gamma':[0.000001, 0.00001,0.0001, 0.001, \n",
    "                                       0.01, 0.1, 1, 10, 100, 1000]}\n",
    "                \n",
    "                # Here, I will select hyperparameters via a systematic \n",
    "                # gridsearch, with a cross validation score of 10 and \n",
    "                # scoring method set to accuracy:\n",
    "                GridSearchcvRBF = GridSearchCV(estimator = classifierRbf, \n",
    "                                               param_grid = param_grid, \n",
    "                                               scoring='accuracy', \n",
    "                                               cv = 10, n_jobs = -1)  \n",
    "                GridSearchcvRBF.fit(X_train, Y_train)\n",
    "                \n",
    "                # Creating a classifier that will take in the gridsearch as \n",
    "                # its regularization parameter to find the best parameters \n",
    "                # for C.\n",
    "                classifier_svmRBF = \n",
    "                svm.SVC(C = GridSearchcvRBF.best_params_['C'], \n",
    "                        kernel = 'rbf')\n",
    "                \n",
    "                # Creating testing and training precitions based on my fitted \n",
    "                # X and Y trains.\n",
    "                classifier_svmRBF.fit(X_train, Y_train)\n",
    "                prediction_svmRBF = classifier_svmRBF.predict(X_test)\n",
    "                prediction_svmRBFtr = classifier_svmRBF.predict(X_train)\n",
    "                \n",
    "                # Printing the training and testing classification reports.\n",
    "                print(\"Classification report of Test RBF SVM:\", \n",
    "                      classification_report(Y_test, prediction_svmRBF))\n",
    "                print(\"Classification report of Train RBF SVM:\", \n",
    "                      classification_report(Y_train, prediction_svmRBFtr))\n",
    "            \n",
    "                # The last one will be an SVM algorithm with a sigmoid kernel.\n",
    "                # Creating a SVM classifier with a sigmoid kernel.\n",
    "                classifierSig = svm.SVC(kernel = 'sigmoid') \n",
    "                \n",
    "                # I will use the same C values and gamma values as the \n",
    "                # CNM06 paper.\n",
    "                C_Values = [0.001,0.005,0.01,0.05,0.1,0.5,1,2] \n",
    "                gamma_list = [0.000001, 0.00001,0.0001, 0.001, 0.01, \n",
    "                              0.1, 1, 10, 100, 1000] \n",
    "                \n",
    "                # Creating parameter grid with C and gamma Values\n",
    "                param_grid = {'C': [0.001,0.005,0.01,0.05,0.1,0.5,1,2], \n",
    "                              'gamma':[0.000001, 0.00001,0.0001, 0.001, \n",
    "                                       0.01, 0.1, 1, 10, 100, 1000]}\n",
    "                \n",
    "                # Here, I will select hyperparameters via a systematic \n",
    "                # gridsearch, with a cross validation score of 10 and \n",
    "                # scoring method set to accuracy:\n",
    "                GridSearchcvSIG = GridSearchCV(estimator = classifierSig, \n",
    "                                               param_grid = param_grid, \n",
    "                                               scoring='accuracy', \n",
    "                                               cv = 10, n_jobs = -1)  \n",
    "                GridSearchcvSIG.fit(X_train, Y_train)\n",
    "                \n",
    "                # Creating a classifier that will take in the gridsearch as \n",
    "                # its regularization parameter to find the best parameters \n",
    "                # for C.\n",
    "                classifier_svmSIG = svm.SVC(C = GridSearchcvSIG.best_params_['C'], kernel = 'sigmoid')\n",
    "                \n",
    "                # Creating testing and training precitions based on my fitted \n",
    "                # X and Y trains.\n",
    "                classifier_svmSIG.fit(X_train, Y_train)\n",
    "                prediction_svmSig = classifier_svmSIG.predict(X_test)\n",
    "                prediction_svmSigtr = classifier_svmSIG.predict(X_train)\n",
    "                \n",
    "                # Printing the training and testing classification reports.\n",
    "                print(\"Classification report of Test Sigmoid SVM:\", \n",
    "                      classification_report(Y_test, prediction_svmSig))\n",
    "                print(\"Classification report of Train Sigmoid SVM:\", \n",
    "                      classification_report(Y_train, prediction_svmSigtr))\n",
    "                \n",
    "                \n",
    "                \n",
    "            ## The third group of algorithms will be Random Forests.\n",
    "            if alg == 2:\n",
    "                # The first will be a Random forest with Gini criterion.\n",
    "                # Setting same depth lengths and estimators as the \n",
    "                # CNM06 paper.\n",
    "                D_list = [1, 2, 4, 6, 8, 12, 16, 20] \n",
    "                n_estimators = 1024\n",
    "                \n",
    "                # Creating a Random Forest classifier with 1024 estimators \n",
    "                # and an gini criterion.\n",
    "                classifierRFgini = \n",
    "                RandomForestClassifier(n_estimators = n_estimators, \n",
    "                                       criterion = 'gini')\n",
    "                \n",
    "                # Here, I will select hyperparameters via a systematic \n",
    "                # gridsearch, with a cross validation score of 10 and \n",
    "                # scoring method set to accuracy:\n",
    "                GridSearchRFgini = GridSearchCV(classifierRFgini, \n",
    "                                                {'max_depth':D_list}, \n",
    "                                                scoring = 'accuracy', \n",
    "                                                cv = 10, n_jobs = -1)\n",
    "                GridSearchRFgini.fit(X_train, Y_train)\n",
    "                \n",
    "                # Creating a classifier that will take in the gridsearch as \n",
    "                # its regularization parameter to find the best parameters \n",
    "                # for C.\n",
    "                classifier_RFgini = \n",
    "                RandomForestClassifier(criterion = 'entropy',\n",
    "                                       max_depth = GridSearchRFgini.best_params_['max_depth'])\n",
    "                \n",
    "                # Creating testing and training precitions based on my fitted \n",
    "                # X and Y trains.\n",
    "                classifier_RFgini.fit(X_train, Y_train)\n",
    "                prediction_RFgini = classifier_RFgini.predict(X_test)\n",
    "                prediction_RFginitr = classifier_RFgini.predict(X_train)\n",
    "                \n",
    "                # Printing the training and testing classification reports.\n",
    "                print(\"Classification report of Test RF (gini):\", \n",
    "                      classification_report(Y_test, prediction_RFgini))\n",
    "                print(\"Classification report of Train RF (gini):\", \n",
    "                      classification_report(Y_train, prediction_RFginitr))\n",
    "            \n",
    "                #The second will be a Random forest with an Entropy criterion.\n",
    "                # Setting same depth lengths and estimators as the \n",
    "                # CNM06 paper.\n",
    "                D_list = [1, 2, 4, 6, 8, 12, 16, 20] \n",
    "                n_estimators = 1024\n",
    "                \n",
    "                # Creating a Random Forest classifier with 1024 estimators \n",
    "                # and an entropy criterion.\n",
    "                classifierRFent = \n",
    "                RandomForestClassifier(n_estimators = n_estimators, \n",
    "                                       criterion = 'entropy')\n",
    "                \n",
    "                # Here, I will select hyperparameters via a systematic \n",
    "                # gridsearch, with a cross validation score of 10 and \n",
    "                # scoring method set to accuracy:\n",
    "                GridSearchRFent = \n",
    "                GridSearchCV(classifierRFent, \n",
    "                             {'max_depth':D_list}, \n",
    "                             scoring = 'accuracy', \n",
    "                             cv = 10, n_jobs = -1)\n",
    "                GridSearchRFent.fit(X_train, Y_train)\n",
    "                \n",
    "                # Creating a classifier that will take in the gridsearch \n",
    "                # as its regularization parameter to find the best parameters \n",
    "                # for C.\n",
    "                classifier_RFent = \n",
    "                RandomForestClassifier(criterion = 'entropy',\n",
    "                                       max_depth = GridSearchRFent.best_params_['max_depth'])\n",
    "                \n",
    "                # Creating testing and training precitions based on my fitted \n",
    "                # X and Y trains.\n",
    "                classifier_RFent.fit(X_train, Y_train)\n",
    "                prediction_RFent = classifier_RFent.predict(X_test)\n",
    "                prediction_RFenttr = classifier_RFent.predict(X_train)\n",
    "                \n",
    "                # Printing the training and testing classification reports.\n",
    "                print(\"Classification report of Test RF (entropy):\", \n",
    "                      classification_report(Y_test, prediction_RFent))\n",
    "                print(\"Classification report of Train RF (entropy):\", \n",
    "                      classification_report(Y_train, prediction_RFenttr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Letter Recognition Dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following code in this block is for my Letter Dataset:\n",
    "\n",
    "# We begin by making a massive for-loop to go through 10 trials.\n",
    "for iterations in range(10):\n",
    "    \n",
    "    # Shuffling the dataset.\n",
    "    letterData = shuffle(letterData) \n",
    "    \n",
    "    # Setting up X values to be everything except the letter column, \n",
    "    # and Y values to be the letter column.\n",
    "    X = letterData.drop('letter', axis = 1)\n",
    "    Y = letterData['letter']\n",
    "    \n",
    "    # Printing the number of each iteration as we go up. \n",
    "    print(\"Trial Number:\", iterations + 1) \n",
    "\n",
    "    \n",
    "    # Setting up a for loop within my iterations loop, which will split my \n",
    "    # data 4 times, in different ways.\n",
    "    for split in range(4):\n",
    "        if split == 0:\n",
    "            # For the first split, I will do a 80(training)/20(testing) \n",
    "            # split, with a Random State of 50.\n",
    "            X_train, X_test, Y_train, Y_test = \n",
    "            train_test_split(X, Y, test_size = 0.2, random_state = 50)\n",
    "            \n",
    "            # Standardizing features by removing the mean and scaling to a \n",
    "            # unit variance.\n",
    "            stsc = StandardScaler() \n",
    "            \n",
    "            # Transforming X train and X test based on the \n",
    "            # standardization above.\n",
    "            X_train = stsc.fit_transform(X_train)\n",
    "            X_test = stsc.fit_transform(X_test)\n",
    "            print(\"Data Split 80/20:\")\n",
    "        \n",
    "        if split == 1:\n",
    "            # For the second split, I will do a 70(training)/30(testing)\n",
    "            # split, with a Random State of 50.\n",
    "            X_train, X_test, Y_train, Y_test = \n",
    "            train_test_split(X, Y, test_size = 0.3, random_state = 50)\n",
    "            \n",
    "            # Standardizing features by removing the mean and scaling to a \n",
    "            # unit variance.\n",
    "            stsc = StandardScaler() \n",
    "            \n",
    "            # Transforming X train and X test based on the \n",
    "            # standardization above.\n",
    "            X_train = stsc.fit_transform(X_train) \n",
    "            X_test = stsc.fit_transform(X_test)\n",
    "            print(\"Data Split 70/30:\")\n",
    "\n",
    "        if split == 2:\n",
    "            # For the third split, I will do a 60(training)/40(testing) \n",
    "            # split, with a Random State of 50.\n",
    "            X_train, X_test, Y_train, Y_test = \n",
    "            train_test_split(X, Y, test_size = 0.4, random_state = 50)\n",
    "            \n",
    "            # Standardizing features by removing the mean and scaling to a \n",
    "            # unit variance.\n",
    "            stsc = StandardScaler() \n",
    "            \n",
    "            # Transforming X train and X test based on the \n",
    "            # standardization above.\n",
    "            X_train = stsc.fit_transform(X_train) \n",
    "            X_test = stsc.fit_transform(X_test)\n",
    "            print(\"Data Split 60/40:\")\n",
    "            \n",
    "        if split == 3:\n",
    "            # For the fourth split, I will do a 50(training)/50(testing) \n",
    "            # split, with a Random State of 50.\n",
    "            X_train, X_test, Y_train, Y_test = \n",
    "            train_test_split(X, Y, test_size = 0.5, random_state = 50)\n",
    "            \n",
    "            # Standardizing features by removing the mean and scaling to a \n",
    "            # unit variance.\n",
    "            stsc = StandardScaler()\n",
    "        \n",
    "            # Transforming X train and X test based on the \n",
    "            # standardization above.\n",
    "            X_train = stsc.fit_transform(X_train) \n",
    "            X_test = stsc.fit_transform(X_test) \n",
    "            print(\"Data Split 50/50:\")\n",
    "\n",
    "            \n",
    "        # Setting up a for-loop for my 3 algorithms. \n",
    "        for alg in range(3):\n",
    "            ## The first group of algorithms will be Decision Trees .\n",
    "            if alg == 0:\n",
    "                # The first will be a Decision Tree with entropy.\n",
    "                # Setting a list for depth length and creating a Classifier \n",
    "                # with entropy.\n",
    "                Depth_list = [1, 4, 8, 16] \n",
    "                classifierENT = tree.DecisionTreeClassifier(criterion = \n",
    "                                                            'entropy') \n",
    "                \n",
    "                # Here, I will select hyperparameters via a systematic \n",
    "                # gridsearch, with a cross validation score of 10 and \n",
    "                # scoring method set to accuracy:\n",
    "                GridSearchENT = GridSearchCV(classifierENT, \n",
    "                                             {'max_depth':Depth_list},\n",
    "                                             scoring = 'accuracy', \n",
    "                                             cv = 10, n_jobs = -1)\n",
    "                GridSearchENT.fit(X_train, Y_train)\n",
    "                \n",
    "                # Creating a classifier that will take in the GridSearchTree \n",
    "                # as its Max Depth parameter to find it the best \n",
    "                # parameters.\n",
    "                classifier_ENT = \n",
    "                tree.DecisionTreeClassifier(criterion = 'entropy', \n",
    "                                            max_depth = GridSearchENT.best_params_['max_depth'])\n",
    "    \n",
    "                # Creating testing and training precitions based on my \n",
    "                # fitted X and Y trains.\n",
    "                classifier_ENT.fit(X_train, Y_train) \n",
    "                prediction_ENT = classifier_ENT.predict(X_test) \n",
    "                prediction_ENTtr = classifier_ENT.predict(X_train)\n",
    "                \n",
    "                # Printing the training and testing classification reports.\n",
    "                print(\"Classification report of Test DT (entropy):\", \n",
    "                      classification_report(Y_test, prediction_ENT))\n",
    "                print(\"Classification report of Train DT (entropy):\", \n",
    "                      classification_report(Y_train, prediction_ENTtr))\n",
    "                \n",
    "                # The second will be a Decision Tree with gini.\n",
    "                # Setting a list for depth length and creating a Classifier \n",
    "                # with gini.\n",
    "                Depth_list = [1, 4, 8, 16] \n",
    "                classifierGINI = tree.DecisionTreeClassifier(criterion = \n",
    "                                                             'gini') \n",
    "               \n",
    "                # Here, I will select hyperparameters via a systematic \n",
    "                # gridsearch, with a cross validation score of 10 and \n",
    "                # scoring method set to accuracy:\n",
    "                GridSearchGINI = GridSearchCV(classifierGINI, \n",
    "                                              {'max_depth':Depth_list}, \n",
    "                                              scoring = 'accuracy', \n",
    "                                              cv = 10, n_jobs = -1)\n",
    "                GridSearchGINI.fit(X_train, Y_train)\n",
    "                \n",
    "                # Creating a classifier that will take in the GridSearchTree\n",
    "                # as its Max Depth parameter to find it the best \n",
    "                # parameters.\n",
    "                classifier_GINI = \n",
    "                tree.DecisionTreeClassifier(criterion = 'entropy', \n",
    "                                            max_depth = GridSearchGINI.best_params_['max_depth'])\n",
    "                \n",
    "                # Creating testing and training precitions based on my fitted \n",
    "                # X and Y trains.\n",
    "                classifier_GINI.fit(X_train, Y_train) \n",
    "                prediction_GINI = classifier_GINI.predict(X_test) \n",
    "                prediction_GINItr = classifier_GINI.predict(X_train) \n",
    "                \n",
    "                # Printing the training and testing classification reports.\n",
    "                print(\"Classification report of Test DT (gini):\", \n",
    "                      classification_report(Y_test, prediction_GINI))\n",
    "                print(\"Classification report of Train DT (gini):\", \n",
    "                      classification_report(Y_train, prediction_GINItr))\n",
    "                \n",
    "                \n",
    "                \n",
    "            ## The second group of algorithms will be different kernels \n",
    "            # variations of the SVM algorithm.\n",
    "            if alg == 1:\n",
    "                # The first will be an SVM algorithm with a linear kernel.\n",
    "                # Creating a SVM classifier with linear kernel.\n",
    "                classifierLin = svm.SVC(kernel = 'linear') \n",
    "                \n",
    "                # I will use the same C values and gamma values as the \n",
    "                # CNM06 paper.\n",
    "                C_Values = [0.001,0.005,0.01,0.05,0.1,0.5,1,2] \n",
    "                gamma_list = [0.000001, 0.00001,0.0001, 0.001, \n",
    "                              0.01, 0.1, 1, 10, 100, 1000]\n",
    "                \n",
    "                # Creating parameter grid with C and gamma Values.\n",
    "                param_grid = {'C': [0.001,0.005,0.01,0.05,0.1,0.5,1,2], \n",
    "                              'gamma':[0.000001, 0.00001,0.0001, 0.001,\n",
    "                                       0.01, 0.1, 1, 10, 100, 1000]}\n",
    "                \n",
    "                # Here, I will select hyperparameters via a systematic \n",
    "                # gridsearch, with a cross validation score of 10 and \n",
    "                # scoring method set to accuracy:\n",
    "                GridSearchcvLin = GridSearchCV(estimator = classifierLin, \n",
    "                                               param_grid = param_grid, \n",
    "                                               scoring='accuracy', \n",
    "                                               cv = 10, n_jobs = -1)  \n",
    "                GridSearchcvLin.fit(X_train, Y_train)\n",
    "                \n",
    "                # Creating a classifier that will take in the gridsearch as \n",
    "                # its regularization parameter to find the best parameters \n",
    "                # for C.\n",
    "                classifier_svmLin = \n",
    "                svm.SVC(C = GridSearchcvLin.best_params_['C'], \n",
    "                        kernel = 'linear')\n",
    "                \n",
    "                # Creating testing and training precitions based on my fitted \n",
    "                # X and Y trains.\n",
    "                classifier_svmLin.fit(X_train, Y_train)\n",
    "                prediction_svmLin = classifier_svmLin.predict(X_test)\n",
    "                prediction_svmLintr = classifier_svmLin.predict(X_train)\n",
    "            \n",
    "                # Printing the training and testing classification reports.\n",
    "                print(\"Classification report of Test Linear SVM:\", \n",
    "                      classification_report(Y_test, prediction_svmLin))\n",
    "                print(\"Classification report of Train Linear SVM:\", \n",
    "                      classification_report(Y_train, prediction_svmLintr))\n",
    "            \n",
    "                # The second will be an SVM algorithm with a rbf kernel\n",
    "                # Creating a SVM classifier with rbf kernel.\n",
    "                classifierRbf = svm.SVC(kernel = 'rbf')\n",
    "                \n",
    "                # I will use the same C values and gamma values as the \n",
    "                # CNM06 paper.\n",
    "                C_Values = [0.001,0.005,0.01,0.05,0.1,0.5,1,2] \n",
    "                gamma_list = [0.000001, 0.00001,0.0001, 0.001, 0.01, \n",
    "                              0.1, 1, 10, 100, 1000] \n",
    "                \n",
    "                # Creating parameter grid with C and gamma Values.\n",
    "                param_grid = {'C': [0.001,0.005,0.01,0.05,0.1,0.5,1,2], \n",
    "                              'gamma':[0.000001, 0.00001,0.0001, 0.001, \n",
    "                                       0.01, 0.1, 1, 10, 100, 1000]}\n",
    "                \n",
    "                # Here, I will select hyperparameters via a systematic \n",
    "                # gridsearch, with a cross validation score of 10 and \n",
    "                # scoring method set to accuracy:\n",
    "                GridSearchcvRBF = GridSearchCV(estimator = classifierRbf, \n",
    "                                               param_grid = param_grid, \n",
    "                                               scoring='accuracy', \n",
    "                                               cv = 10, n_jobs = -1)  \n",
    "                GridSearchcvRBF.fit(X_train, Y_train)\n",
    "                \n",
    "                # Creating a classifier that will take in the gridsearch as \n",
    "                # its regularization parameter to find the best parameters \n",
    "                # for C.\n",
    "                classifier_svmRBF = \n",
    "                svm.SVC(C = GridSearchcvRBF.best_params_['C'], \n",
    "                        kernel = 'rbf')\n",
    "                \n",
    "                # Creating testing and training precitions based on my fitted \n",
    "                # X and Y trains.\n",
    "                classifier_svmRBF.fit(X_train, Y_train)\n",
    "                prediction_svmRBF = classifier_svmRBF.predict(X_test)\n",
    "                prediction_svmRBFtr = classifier_svmRBF.predict(X_train)\n",
    "                \n",
    "                # Printing the training and testing classification reports.\n",
    "                print(\"Classification report of Test RBF SVM:\", \n",
    "                      classification_report(Y_test, prediction_svmRBF))\n",
    "                print(\"Classification report of Train RBF SVM:\", \n",
    "                      classification_report(Y_train, prediction_svmRBFtr))\n",
    "            \n",
    "                # The last one will be an SVM algorithm with a sigmoid kernel.\n",
    "                # Creating a SVM classifier with a sigmoid kernel.\n",
    "                classifierSig = svm.SVC(kernel = 'sigmoid') \n",
    "                \n",
    "                # I will use the same C values and gamma values as the \n",
    "                # CNM06 paper.\n",
    "                C_Values = [0.001,0.005,0.01,0.05,0.1,0.5,1,2] \n",
    "                gamma_list = [0.000001, 0.00001,0.0001, 0.001, 0.01, \n",
    "                              0.1, 1, 10, 100, 1000] \n",
    "                \n",
    "                # Creating parameter grid with C and gamma Values\n",
    "                param_grid = {'C': [0.001,0.005,0.01,0.05,0.1,0.5,1,2], \n",
    "                              'gamma':[0.000001, 0.00001,0.0001, 0.001, \n",
    "                                       0.01, 0.1, 1, 10, 100, 1000]}\n",
    "                \n",
    "                # Here, I will select hyperparameters via a systematic \n",
    "                # gridsearch, with a cross validation score of 10 and \n",
    "                # scoring method set to accuracy:\n",
    "                GridSearchcvSIG = GridSearchCV(estimator = classifierSig, \n",
    "                                               param_grid = param_grid, \n",
    "                                               scoring='accuracy', \n",
    "                                               cv = 10, n_jobs = -1)  \n",
    "                GridSearchcvSIG.fit(X_train, Y_train)\n",
    "                \n",
    "                # Creating a classifier that will take in the gridsearch as \n",
    "                # its regularization parameter to find the best parameters \n",
    "                # for C.\n",
    "                classifier_svmSIG = svm.SVC(C = GridSearchcvSIG.best_params_['C'], kernel = 'sigmoid')\n",
    "                \n",
    "                # Creating testing and training precitions based on my fitted \n",
    "                # X and Y trains.\n",
    "                classifier_svmSIG.fit(X_train, Y_train)\n",
    "                prediction_svmSig = classifier_svmSIG.predict(X_test)\n",
    "                prediction_svmSigtr = classifier_svmSIG.predict(X_train)\n",
    "                \n",
    "                # Printing the training and testing classification reports.\n",
    "                print(\"Classification report of Test Sigmoid SVM:\", \n",
    "                      classification_report(Y_test, prediction_svmSig))\n",
    "                print(\"Classification report of Train Sigmoid SVM:\", \n",
    "                      classification_report(Y_train, prediction_svmSigtr))\n",
    "                \n",
    "                \n",
    "                \n",
    "            ## The third group of algorithms will be Random Forests.\n",
    "            if alg == 2:\n",
    "                # The first will be a Random forest with Gini criterion.\n",
    "                # Setting same depth lengths and estimators as the \n",
    "                # CNM06 paper.\n",
    "                D_list = [1, 2, 4, 6, 8, 12, 16, 20] \n",
    "                n_estimators = 1024\n",
    "                \n",
    "                # Creating a Random Forest classifier with 1024 estimators \n",
    "                # and an gini criterion.\n",
    "                classifierRFgini = \n",
    "                RandomForestClassifier(n_estimators = n_estimators, \n",
    "                                       criterion = 'gini')\n",
    "                \n",
    "                # Here, I will select hyperparameters via a systematic \n",
    "                # gridsearch, with a cross validation score of 10 and \n",
    "                # scoring method set to accuracy:\n",
    "                GridSearchRFgini = GridSearchCV(classifierRFgini, \n",
    "                                                {'max_depth':D_list}, \n",
    "                                                scoring = 'accuracy', \n",
    "                                                cv = 10, n_jobs = -1)\n",
    "                GridSearchRFgini.fit(X_train, Y_train)\n",
    "                \n",
    "                # Creating a classifier that will take in the gridsearch as \n",
    "                # its regularization parameter to find the best parameters \n",
    "                # for C.\n",
    "                classifier_RFgini = \n",
    "                RandomForestClassifier(criterion = 'entropy',\n",
    "                                       max_depth = GridSearchRFgini.best_params_['max_depth'])\n",
    "                \n",
    "                # Creating testing and training precitions based on my fitted \n",
    "                # X and Y trains.\n",
    "                classifier_RFgini.fit(X_train, Y_train)\n",
    "                prediction_RFgini = classifier_RFgini.predict(X_test)\n",
    "                prediction_RFginitr = classifier_RFgini.predict(X_train)\n",
    "                \n",
    "                # Printing the training and testing classification reports.\n",
    "                print(\"Classification report of Test RF (gini):\", \n",
    "                      classification_report(Y_test, prediction_RFgini))\n",
    "                print(\"Classification report of Train RF (gini):\", \n",
    "                      classification_report(Y_train, prediction_RFginitr))\n",
    "            \n",
    "                #The second will be a Random forest with an Entropy criterion.\n",
    "                # Setting same depth lengths and estimators as the \n",
    "                # CNM06 paper.\n",
    "                D_list = [1, 2, 4, 6, 8, 12, 16, 20] \n",
    "                n_estimators = 1024\n",
    "                \n",
    "                # Creating a Random Forest classifier with 1024 estimators \n",
    "                # and an entropy criterion.\n",
    "                classifierRFent = \n",
    "                RandomForestClassifier(n_estimators = n_estimators, \n",
    "                                       criterion = 'entropy')\n",
    "                \n",
    "                # Here, I will select hyperparameters via a systematic \n",
    "                # gridsearch, with a cross validation score of 10 and \n",
    "                # scoring method set to accuracy:\n",
    "                GridSearchRFent = \n",
    "                GridSearchCV(classifierRFent, \n",
    "                             {'max_depth':D_list}, \n",
    "                             scoring = 'accuracy', \n",
    "                             cv = 10, n_jobs = -1)\n",
    "                GridSearchRFent.fit(X_train, Y_train)\n",
    "                \n",
    "                # Creating a classifier that will take in the gridsearch \n",
    "                # as its regularization parameter to find the best parameters \n",
    "                # for C.\n",
    "                classifier_RFent = \n",
    "                RandomForestClassifier(criterion = 'entropy',\n",
    "                                       max_depth = GridSearchRFent.best_params_['max_depth'])\n",
    "                \n",
    "                # Creating testing and training precitions based on my fitted \n",
    "                # X and Y trains.\n",
    "                classifier_RFent.fit(X_train, Y_train)\n",
    "                prediction_RFent = classifier_RFent.predict(X_test)\n",
    "                prediction_RFenttr = classifier_RFent.predict(X_train)\n",
    "                \n",
    "                # Printing the training and testing classification reports.\n",
    "                print(\"Classification report of Test RF (entropy):\", \n",
    "                      classification_report(Y_test, prediction_RFent))\n",
    "                print(\"Classification report of Train RF (entropy):\", \n",
    "                      classification_report(Y_train, prediction_RFenttr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that I have all of the data printed, I manualy transferred the raw data to .xlsx sheets. (Yes, I had to do this manually and it literally took me 4 days. I wanted to die lol). In excel I also calculated the means per score and/or datasets as it was more convenient to do it there than to import everything here and splice rows/columns to find the same results.\n",
    "\n",
    "What I'll do now is import the testing datasets, essentially tables 2 and 3 from my report, so I can perform t-tests on them, and extract their p-values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performing T-Tests:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Table 2:\n",
      "            Model   Bean    Adult  Letter    Skin   Mean\n",
      "0   DT (Entropy)  0.993*  0.767*  0.742*  0.987*  0.872\n",
      "1      DT (Gini)  0.993*  0.767*  0.741*  0.986*  0.871\n",
      "2   SVM (Linear)   0.992   0.778   0.734   0.954  0.865\n",
      "3      SVM (Rbf)   0.988   0.779   0.942   0.999  0.927\n",
      "4  SVM (Sigmoid)   0.938   0.769   0.695   0.918  0.831\n",
      "5      RF (Gini)   0.993  0.775*  0.817*  0.986*  0.893\n",
      "6   RF (Entropy)  0.993*  0.776*  0.816*  0.985*  0.892\n",
      "\n",
      " Table 3:\n",
      "            Model Precision  Recall F1 Score  Accuracy    Mean\n",
      "0   DT (Entropy)     0.848  0.858*    0.862*    0.887*  0.864\n",
      "1      DT (Gini)     0.879  0.858*    0.863*    0.888*  0.872\n",
      "2   SVM (Linear)     0.862   0.851     0.859     0.879  0.863\n",
      "3      SVM (Rbf)     0.929   0.915     0.921     0.941  0.927\n",
      "4  SVM (Sigmoid)     0.835   0.815     0.823     0.849  0.831\n",
      "5      RF (Gini)    0.898*  0.881*    0.885*    0.908*  0.893\n",
      "6   RF (Entropy)    0.897*  0.881*    0.885*    0.908*  0.893\n"
     ]
    }
   ],
   "source": [
    "# Importing the aformentioned tables to perform analysis on.\n",
    "Table2 = pd.read_excel(\"COGS 118A Table 2.xlsx\")\n",
    "Table3 = pd.read_excel(\"COGS 118A Table 3.xlsx\")\n",
    "print(' Table 2:\\n', Table2)\n",
    "print('\\n Table 3:\\n', Table3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P-Values for Table 2: \n",
      " Bean P-value: Ttest_relResult(statistic=-12.475350446621924, pvalue=1.6212230123026137e-05) \n",
      " Adult P-Value: Ttest_relResult(statistic=10.436346302215146, pvalue=4.537736807503584e-05) \n",
      " Letter P-Value: Ttest_relResult(statistic=4.550685778670225, pvalue=0.0038890148886611396) \n",
      " Skin P-Value: Ttest_relResult(statistic=-16.21764870898495, pvalue=3.4966684329500023e-06) \n",
      "\n",
      "P-Values for Table 3: \n",
      " DT (Entropy) P-value: Ttest_relResult(statistic=2.752988806446741, pvalue=0.07056528599350025) \n",
      " DT (Gini) P-value: Ttest_relResult(statistic=1.1448491654582649, pvalue=0.3353305924290619) \n",
      " SVM (Linear) P-value: Ttest_relResult(statistic=14.240786495134317, pvalue=0.0007502615749642737) \n",
      " SVM (Rbf) P-value: Ttest_relResult(statistic=-26.888006848420112, pvalue=0.00011288508590177278) \n",
      " SVM (Sigmoid) P-value: Ttest_relResult(statistic=17.231584153731482, pvalue=0.0004258479743583426) \n",
      " RF (Gini) P-value: Ttest_relResult(statistic=-6.5689464453451505, pvalue=0.007176091410517835) \n",
      " RF (Entropy) P-value: Ttest_relResult(statistic=-7.1414284285428495, pvalue=0.005652993346948494) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# The following code is for Performing T-Tests:\n",
    "\n",
    "# Performing T-test on table 2:\n",
    "# Transfering data from table 2 to be analyzed.\n",
    "MeanAcross = [.872, .871, .865, .927, .831, .893, .892]\n",
    "BeanMean = [.993, .993, .992, .988, .938, .993, .993]\n",
    "AdultMean = [.767, .767, .778, .779, .769, .775, .776]\n",
    "LetterMean = [.742, .741, .734, .942, .695, .817, .816]\n",
    "SkinMean = [.987, .986, .954, .999, .918, .986, .985]\n",
    "\n",
    "# Performing T-tests:\n",
    "pBean = stats.ttest_rel(MeanAcross, BeanMean)\n",
    "pAdult = stats.ttest_rel(MeanAcross, AdultMean)\n",
    "pLetter = stats.ttest_rel(MeanAcross, LetterMean)\n",
    "pSkin = stats.ttest_rel(MeanAcross,SkinMean)\n",
    "print('P-Values for Table 2:','\\n' ,'Bean P-value:', pBean, '\\n', \n",
    "      'Adult P-Value:', pAdult, '\\n', 'Letter P-Value:', pLetter,'\\n', \n",
    "      'Skin P-Value:', pSkin, '\\n')\n",
    "\n",
    "# Performing T-Test on table 3:\n",
    "# Transfering data from table 3 to be analyzed.\n",
    "Mean = [.873, .865, .871, .894] \n",
    "DT_Ent = [.848, .858, .862, .887]\n",
    "DT_Gini = [.879, .858, .863, .888]\n",
    "SVM_Lin = [.862, .851, .859, .879]\n",
    "SVM_Rbf = [.929, .915, .921, .941]\n",
    "SVM_Sig = [.835, .815, .823, .849]\n",
    "RF_Gini = [.898, .881, .885, .908]\n",
    "RF_Ent = [.897, .881, .885, .908]\n",
    "\n",
    "# Performing T-tests:\n",
    "pDT_Ent = stats.ttest_rel(Mean, DT_Ent)\n",
    "pDT_Gini = stats.ttest_rel(Mean, DT_Gini)\n",
    "pSVM_Lin = stats.ttest_rel(Mean, SVM_Lin)\n",
    "pSVM_Rbf = stats.ttest_rel(Mean, SVM_Rbf)\n",
    "pSVM_Sig = stats.ttest_rel(Mean, SVM_Sig)\n",
    "pRF_Gini = stats.ttest_rel(Mean, RF_Gini)\n",
    "pRF_Ent = stats.ttest_rel(Mean, RF_Ent)\n",
    "print('P-Values for Table 3:','\\n', 'DT (Entropy) P-value:', pDT_Ent, '\\n',\n",
    "                                    'DT (Gini) P-value:', pDT_Gini, '\\n',\n",
    "                                    'SVM (Linear) P-value:', pSVM_Lin, '\\n',\n",
    "                                    'SVM (Rbf) P-value:', pSVM_Rbf, '\\n',\n",
    "                                    'SVM (Sigmoid) P-value:', pSVM_Sig, '\\n',\n",
    "                                    'RF (Gini) P-value:', pRF_Gini, '\\n',\n",
    "                                    'RF (Entropy) P-value:', pRF_Ent, '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This next block of code is for a heatmap of mean algorithm eprformances per data partition. Once again, I calculated the values in google sheets where my raw data was, so I'm just going to transfer them ober to a numpy array here. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x360 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Creating an arraw where columns are data splits and rows are algorithms, \n",
    "# storing their performance per split. I calculated all these in google \n",
    "# sheets where my raw code is stored.\n",
    "Metric_Scores_By_Partition = np.array([[0.872, 0.874,0.871,0.869],\n",
    "                                       [0.872, 0.873,0.872,0.870],\n",
    "                                       [0.865, 0.864,0.865,0.862],\n",
    "                                       [0.933, 0.930,0.929,0.925],\n",
    "                                       [0.833, 0.831,0.832,0.830],\n",
    "                                       [0.895, 0.894,0.893,0.891],\n",
    "                                       [0.893, 0.894,0.892,0.891]])\n",
    "                                      \n",
    "# Creating labels for the rows.\n",
    "labels = np.array(['DT (Entropy)', 'DT(Gini)', 'SVM (Linear)', 'SVM (Rbf)', \n",
    "                   'SVM (Sigmoid)', 'RF (Gini)', 'RF (Entropy)'])\n",
    "\n",
    "# Sorting data for the heatmap.\n",
    "order = np.argsort(np.mean(Metric_Scores_By_Partition, axis = 1))[::-1]\n",
    "\n",
    "# Creating the heatmap.\n",
    "plt.subplots(figsize = (5,5))\n",
    "graph = sns.heatmap(Metric_Scores_By_Partition[order], cmap = 'plasma', \n",
    "                    annot = True, fmt = '.4f', \n",
    "                    xticklabels = ['80/20', '70/30', '60/40', '50/50'], \n",
    "                    linewidths = 0.5, cbar = True)\n",
    "graph.set_yticklabels(labels[order], rotation = 360)\n",
    "plt.subplots_adjust(left = 0, bottom = 0, \n",
    "                    right = 1, top = 1, \n",
    "                    wspace = 0, hspace = 0)\n",
    "plt.xlabel('Partitions')\n",
    "plt.ylabel('Algorithms')\n",
    "plt.savefig('average_score.png', dpi = 300)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
